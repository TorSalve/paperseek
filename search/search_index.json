{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PaperSeek","text":"<p>A production-ready Python package providing a unified interface for searching multiple academic databases with comprehensive features including rate limiting, field tracking, and multiple export formats.</p>"},{"location":"#overview","title":"Overview","text":"<p>PaperSeek provides a single, consistent API to search across 9 major academic databases:</p> <ul> <li>CrossRef - Comprehensive metadata for scholarly publications</li> <li>OpenAlex - Open catalog of scholarly papers, authors, and institutions</li> <li>Semantic Scholar - AI-powered academic search with citation context</li> <li>DOI.org - DOI resolution service</li> <li>PubMed - Biomedical and life sciences literature (36M+ citations)</li> <li>arXiv - Physics, mathematics, computer science preprints</li> <li>CORE - World's largest collection of open access papers</li> <li>Unpaywall - Open access article finder</li> <li>DBLP - Computer science bibliography</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#unified-search-interface","title":"\ud83d\udd0d Unified Search Interface","text":"<p>Search across multiple databases with a single API call, with automatic result merging and deduplication.</p>"},{"location":"#field-tracking-statistics","title":"\ud83d\udcca Field Tracking &amp; Statistics","text":"<p>Track field availability across results, generate coverage reports, and filter by required fields.</p>"},{"location":"#polite-rate-limiting","title":"\u23f1\ufe0f Polite Rate Limiting","text":"<p>Configurable per-database rate limits with automatic retry logic and exponential backoff.</p>"},{"location":"#multiple-export-formats","title":"\ud83d\udcc4 Multiple Export Formats","text":"<p>Export results to CSV, JSON, JSONL, or BibTeX with customizable fields and streaming support.</p>"},{"location":"#pdf-downloader","title":"\ud83d\udce5 PDF Downloader","text":"<p>Conservative, polite PDF downloader for open access papers with progress tracking and verification.</p>"},{"location":"#flexible-fallback-modes","title":"\ud83d\udd04 Flexible Fallback Modes","text":"<p>Configure fallback behavior between databases: sequential, parallel, or first-only.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from paperseek import UnifiedSearchClient\n\n# Initialize client with multiple databases\nclient = UnifiedSearchClient(databases=[\"crossref\", \"openalex\", \"semantic_scholar\"])\n\n# Search for papers\nresults = client.search(\n    query=\"machine learning\",\n    year=2023,\n    max_results=100\n)\n\n# Access results\nprint(f\"Found {len(results)} papers\")\nfor paper in results.papers[:5]:\n    print(f\"- {paper.title}\")\n    print(f\"  Authors: {', '.join(a.name for a in paper.authors[:3])}\")\n    print(f\"  Citations: {paper.citation_count}\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install paperseek\n</code></pre> <p>Or install from source:</p> <pre><code>git clone https://github.com/TorSalve/paperseek.git\ncd paperseek\npip install -e .\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed installation instructions</li> <li>Quick Start - Get started in 5 minutes</li> <li>Database Overview - Learn about each database</li> <li>API Reference - Complete API documentation</li> <li>Examples - Code examples for common tasks</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This document describes the internal architecture of Academic Search Unified.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>The package is designed with modularity and extensibility in mind, following object-oriented principles and clean architecture patterns.</p>"},{"location":"architecture/#package-structure","title":"Package Structure","text":"<pre><code>src/academic_search_unified/\n\u251c\u2500\u2500 __init__.py                 # Package exports\n\u251c\u2500\u2500 core/                       # Core functionality\n\u2502   \u251c\u2500\u2500 base.py                # Base client interface\n\u2502   \u251c\u2500\u2500 exceptions.py          # Custom exceptions\n\u2502   \u251c\u2500\u2500 models.py              # Pydantic data models\n\u2502   \u2514\u2500\u2500 unified_client.py      # Main unified client\n\u251c\u2500\u2500 clients/                    # Database clients\n\u2502   \u251c\u2500\u2500 crossref.py\n\u2502   \u251c\u2500\u2500 openalex.py\n\u2502   \u251c\u2500\u2500 semantic_scholar.py\n\u2502   \u251c\u2500\u2500 doi.py\n\u2502   \u251c\u2500\u2500 pubmed.py\n\u2502   \u251c\u2500\u2500 arxiv.py\n\u2502   \u251c\u2500\u2500 core.py\n\u2502   \u251c\u2500\u2500 unpaywall.py\n\u2502   \u2514\u2500\u2500 dblp.py\n\u251c\u2500\u2500 utils/                      # Utilities\n\u2502   \u251c\u2500\u2500 config.py              # Configuration management\n\u2502   \u251c\u2500\u2500 rate_limiter.py        # Rate limiting\n\u2502   \u2514\u2500\u2500 pdf_downloader.py      # PDF downloading\n\u2514\u2500\u2500 config/                     # Default configuration\n    \u2514\u2500\u2500 databases.yaml         # Database defaults\n</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#base-client-databaseclient","title":"Base Client (<code>DatabaseClient</code>)","text":"<p>Abstract base class that defines the interface all database clients must implement:</p> <pre><code>class DatabaseClient(ABC):\n    @abstractmethod\n    def search(self, filters: SearchFilters) -&gt; SearchResult:\n        \"\"\"Search the database.\"\"\"\n        pass\n\n    @abstractmethod\n    def lookup_doi(self, doi: str) -&gt; Optional[Paper]:\n        \"\"\"Look up a paper by DOI.\"\"\"\n        pass\n</code></pre> <p>Key features: - Rate limiting integration - Retry logic with exponential backoff - Consistent error handling - HTTP session management</p>"},{"location":"architecture/#data-models-modelspy","title":"Data Models (<code>models.py</code>)","text":"<p>Pydantic models ensure type safety and validation:</p> <ul> <li><code>Paper</code>: Normalized paper metadata with 20+ fields</li> <li><code>Author</code>: Author information with optional ORCID</li> <li><code>SearchResult</code>: Collection of papers with metadata</li> <li><code>SearchFilters</code>: Query parameters and constraints</li> </ul> <p>Benefits: - Automatic validation - Type hints for IDE support - Easy serialization/deserialization - Immutable by default (with frozen authors)</p>"},{"location":"architecture/#unified-client-unifiedsearchclient","title":"Unified Client (<code>UnifiedSearchClient</code>)","text":"<p>Orchestrates searches across multiple databases:</p> <ol> <li>Initialization: Loads configuration and creates client instances</li> <li>Search: Routes queries to appropriate databases</li> <li>Merging: Combines results from multiple sources</li> <li>Deduplication: Removes duplicate papers based on DOI/title</li> </ol> <p>Fallback modes: - Sequential: Try databases one by one - Parallel: Query all simultaneously - First-only: Use only the first enabled database</p>"},{"location":"architecture/#database-clients","title":"Database Clients","text":"<p>Each client implements the <code>DatabaseClient</code> interface:</p>"},{"location":"architecture/#client-responsibilities","title":"Client Responsibilities","text":"<ol> <li>API Communication: HTTP requests with proper headers</li> <li>Response Parsing: Convert API responses to <code>Paper</code> objects</li> <li>Field Mapping: Map database-specific fields to unified schema</li> <li>Error Handling: Handle API errors gracefully</li> <li>Rate Limiting: Respect API rate limits</li> </ol>"},{"location":"architecture/#example-crossref-client","title":"Example: CrossRef Client","text":"<pre><code>class CrossRefClient(DatabaseClient):\n    def search(self, filters: SearchFilters) -&gt; SearchResult:\n        # Build query parameters\n        params = self._build_query_params(filters)\n\n        # Make API request with rate limiting\n        response = self._make_request(\"/works\", params)\n\n        # Parse response\n        papers = [self._parse_paper(item) for item in response[\"items\"]]\n\n        return SearchResult(papers=papers, databases_queried=[\"crossref\"])\n\n    def _parse_paper(self, data: dict) -&gt; Paper:\n        # Map CrossRef fields to unified schema\n        return Paper(\n            doi=data.get(\"DOI\"),\n            title=data.get(\"title\", [\"\"])[0],\n            authors=self._parse_authors(data.get(\"author\", [])),\n            # ... more field mappings\n        )\n</code></pre>"},{"location":"architecture/#utility-components","title":"Utility Components","text":""},{"location":"architecture/#rate-limiter","title":"Rate Limiter","text":"<p>Two implementations:</p> <ol> <li><code>RateLimiter</code>: Uses <code>pyrate-limiter</code> for precise rate limiting</li> <li><code>SimpleRateLimiter</code>: Fallback using sliding window</li> </ol> <p>Features: - Per-second and per-minute limits - Thread-safe operation - Automatic waiting when limits reached</p>"},{"location":"architecture/#pdf-downloader","title":"PDF Downloader","text":"<p>Conservative PDF downloading with:</p> <ol> <li>Verification: Check content type and file size before download</li> <li>Rate Limiting: Polite delays between downloads (default 3s)</li> <li>Organization: Automatic directory structure</li> <li>Statistics: Track success/failure rates</li> </ol> <p>Flow: <pre><code>Paper \u2192 Check PDF URL \u2192 HEAD request \u2192 Verify \u2192 Download \u2192 Save\n</code></pre></p>"},{"location":"architecture/#configuration-manager","title":"Configuration Manager","text":"<p>Hierarchical configuration loading:</p> <ol> <li>Default values (in code)</li> <li>Package defaults (<code>config/databases.yaml</code>)</li> <li>User config file (<code>config.yaml</code>)</li> <li>Environment variables (<code>.env</code>)</li> <li>Runtime config dict</li> </ol> <p>Priority: Runtime &gt; Env vars &gt; User config &gt; Package defaults &gt; Code defaults</p>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#search-flow","title":"Search Flow","text":"<pre><code>User Query\n    \u2193\nSearchFilters\n    \u2193\nUnifiedSearchClient\n    \u2193\nDatabase Clients (sequential/parallel)\n    \u2193\nAPI Requests (with rate limiting)\n    \u2193\nRaw API Responses\n    \u2193\nPaper Objects (parsed and normalized)\n    \u2193\nSearchResult (merged and deduplicated)\n    \u2193\nUser\n</code></pre>"},{"location":"architecture/#export-flow","title":"Export Flow","text":"<pre><code>SearchResult\n    \u2193\nFormat Converter (CSV/JSON/BibTeX)\n    \u2193\nFile Writer (with optional streaming)\n    \u2193\nOutput File\n</code></pre>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#adding-a-new-database","title":"Adding a New Database","text":"<ol> <li>Create new client class in <code>clients/</code>:</li> </ol> <pre><code>from paperseek.core.base import DatabaseClient\n\nclass NewDatabaseClient(DatabaseClient):\n    def __init__(self, config: dict):\n        super().__init__(\"newdb\", config)\n\n    def search(self, filters: SearchFilters) -&gt; SearchResult:\n        # Implement search logic\n        pass\n\n    def lookup_doi(self, doi: str) -&gt; Optional[Paper]:\n        # Implement DOI lookup\n        pass\n</code></pre> <ol> <li>Register in <code>UnifiedSearchClient</code>:</li> </ol> <pre><code>DATABASE_CLIENTS = {\n    \"crossref\": CrossRefClient,\n    \"openalex\": OpenAlexClient,\n    \"newdb\": NewDatabaseClient,  # Add here\n}\n</code></pre> <ol> <li>Add default configuration in <code>config/databases.yaml</code></li> </ol>"},{"location":"architecture/#adding-export-formats","title":"Adding Export Formats","text":"<p>Add methods to <code>SearchResult</code>:</p> <pre><code>def to_custom_format(self, filename: str) -&gt; None:\n    \"\"\"Export to custom format.\"\"\"\n    # Implementation\n</code></pre>"},{"location":"architecture/#design-principles","title":"Design Principles","text":""},{"location":"architecture/#1-single-responsibility","title":"1. Single Responsibility","text":"<p>Each class has one clear purpose: - Clients handle API communication - Models handle data structure - Utilities handle cross-cutting concerns</p>"},{"location":"architecture/#2-openclosed-principle","title":"2. Open/Closed Principle","text":"<p>Open for extension (new databases), closed for modification (core logic stable).</p>"},{"location":"architecture/#3-dependency-inversion","title":"3. Dependency Inversion","text":"<p>High-level <code>UnifiedSearchClient</code> depends on abstract <code>DatabaseClient</code>, not concrete implementations.</p>"},{"location":"architecture/#4-composition-over-inheritance","title":"4. Composition Over Inheritance","text":"<p>Clients use composition (rate limiters, HTTP sessions) rather than deep inheritance.</p>"},{"location":"architecture/#5-fail-safe-defaults","title":"5. Fail-Safe Defaults","text":"<p>Conservative defaults: low rate limits, polite delays, safe file operations.</p>"},{"location":"architecture/#error-handling","title":"Error Handling","text":"<p>Hierarchical exception system:</p> <pre><code>AcademicSearchException (base)\n\u251c\u2500\u2500 APIException\n\u2502   \u251c\u2500\u2500 RateLimitException\n\u2502   \u251c\u2500\u2500 AuthenticationException\n\u2502   \u2514\u2500\u2500 NetworkException\n\u2514\u2500\u2500 ValidationException\n</code></pre> <p>Strategy: - Catch specific exceptions close to source - Log errors with context - Return partial results when possible - Never crash on single database failure</p>"},{"location":"architecture/#testing-strategy","title":"Testing Strategy","text":"<ul> <li>Unit tests: Test individual components in isolation</li> <li>Integration tests: Test database clients with real APIs (rate-limited)</li> <li>Mock tests: Test unified client with mocked database responses</li> <li>Type checking: Use mypy in standard mode</li> </ul>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Connection pooling: Reuse HTTP sessions</li> <li>Rate limiting: Prevent API throttling</li> <li>Streaming: Handle large result sets efficiently</li> <li>Deduplication: Use hash sets for O(1) lookup</li> <li>Lazy loading: Parse fields only when accessed</li> </ul>"},{"location":"architecture/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements:</p> <ul> <li>Async support: Use <code>asyncio</code> for parallel queries</li> <li>Caching: Cache results to reduce API calls</li> <li>Database: Store results in SQLite/PostgreSQL</li> <li>Web interface: Flask/FastAPI dashboard</li> <li>Monitoring: Prometheus metrics for API usage</li> </ul>"},{"location":"configuration/","title":"Configuration Guide# General settings","text":"<p>email: your.email@example.com  # For polite API requests</p> <p>This guide covers all configuration options for Academic Search Unified.user_agent: AcademicSearchUnified/0.1.0</p> <p>log_level: INFO</p>"},{"location":"configuration/#configuration-methods-log_file-academic_searchlog-optional-log-to-file","title":"Configuration Methods# log_file: academic_search.log  # Optional: log to file","text":"<p>You can configure the client in three ways:# Default search settings</p> <p>default_max_results: 100</p>"},{"location":"configuration/#1-configuration-file-recommendeddefault_timeout-30","title":"1. Configuration File (Recommended)default_timeout: 30","text":"<p>Create a <code>config.yaml</code> file:# Fallback behavior</p> <p>fallback_mode: sequential  # Options: sequential, parallel, first</p> <p>```yamlfail_fast: false  # If true, stop on first error</p>"},{"location":"configuration/#database-configuration","title":"Database configuration","text":"<p>databases:# Database configurations</p> <p>crossref:crossref:</p> <pre><code>enabled: true  enabled: true\n\nrate_limit_per_second: 1.0  # api_key: optional_api_key  # Not required for CrossRef\n\nrate_limit_per_minute: 50  rate_limit_per_second: 1.0\n\ntimeout: 30\n</code></pre> <p>openalex:  max_retries: 3</p> <pre><code>enabled: true  retry_delay: 1.0\n\nemail: \"your.email@example.com\"  # Polite requests\n\nrate_limit_per_second: 10.0openalex:\n\nenabled: true\n</code></pre> <p>semantic_scholar:  # api_key: your_openalex_api_key  # Optional: For Premium users only</p> <pre><code>enabled: true  # Email is automatically added via mailto parameter for polite pool access\n\napi_key: \"your_api_key_here\"  # Optional but recommended  # Free tier with polite pool: ~10 req/sec, Premium tier: higher limits + special filters\n\nrate_limit_per_second: 1.0  rate_limit_per_second: 5.0  # Conservative for polite pool (10/sec limit)\n</code></pre> <p>timeout: 30</p>"},{"location":"configuration/#global-settings-max_retries-3","title":"Global settings  max_retries: 3","text":"<p>fallback_mode: \"sequential\"  # Options: sequential, parallel, first_only  retry_delay: 1.0</p> <p>default_max_results: 100</p> <p>deduplication: truesemantic_scholar:</p> <p>```  enabled: true</p> <p>api_key: your_semantic_scholar_api_key  # Recommended for higher limits</p> <p>Load the configuration:  rate_limit_per_second: 1.0  # Without API key: ~1/sec, with key: ~10/sec</p> <p>rate_limit_per_minute: 100</p> <p>```python  timeout: 30</p> <p>from paperseek import UnifiedSearchClient  max_retries: 3</p> <p>retry_delay: 1.0</p> <p>client = UnifiedSearchClient(config_path=\"config.yaml\")</p> <p>```doi:</p> <p>enabled: true</p>"},{"location":"configuration/#2-environment-variables-rate_limit_per_second-20","title":"2. Environment Variables  rate_limit_per_second: 2.0","text":"<p>timeout: 30</p> <p>Create a <code>.env</code> file:  max_retries: 3</p> <p>retry_delay: 1.0</p> <pre><code># API Keyspubmed:\n\nSEMANTIC_SCHOLAR_API_KEY=your_s2_api_key  enabled: true\n\nCORE_API_KEY=your_core_api_key  # api_key: your_ncbi_api_key  # Optional: For 10 req/sec (vs 3 req/sec without)\n\n  # Email is recommended by NCBI for tracking usage\n\n# Email addresses (for polite requests)  rate_limit_per_second: 2.5  # Conservative (3/sec without key, 10/sec with key)\n\nOPENALEX_EMAIL=your.email@example.com  timeout: 30\n\nPUBMED_EMAIL=your.email@example.com  max_retries: 3\n\nUNPAYWALL_EMAIL=your.email@example.com  retry_delay: 1.0\n</code></pre> <p>arxiv:</p> <p>The client will automatically load these when initialized.  enabled: true</p> <p># No API key required</p>"},{"location":"configuration/#3-config-dictionary-rate-limiting-1-request-per-3-seconds-recommended-by-arxiv","title":"3. Config Dictionary  # Rate limiting: 1 request per 3 seconds recommended by arXiv","text":"<p>rate_limit_per_second: 0.33  # ~1 request per 3 seconds</p> <p>Pass configuration directly to the client:  timeout: 30</p> <p>max_retries: 3</p> <p>```python  retry_delay: 1.0</p> <p>config = {</p> <pre><code>\"crossref\": {core:\n\n    \"enabled\": True,  enabled: true\n\n    \"rate_limit_per_second\": 1.0  api_key: your_core_api_key  # Required - register at https://core.ac.uk/services/api\n\n},  # Free tier: 10,000 requests/day\n\n\"openalex\": {  rate_limit_per_second: 2.0\n\n    \"enabled\": True,  timeout: 30\n\n    \"email\": \"your.email@example.com\"  max_retries: 3\n\n}  retry_delay: 1.0\n</code></pre> <p>}</p> <p>unpaywall:</p> <p>client = UnifiedSearchClient(  enabled: true</p> <pre><code>databases=[\"crossref\", \"openalex\"],  # No API key required, but email is required\n\nconfig_dict=config  # Free for non-commercial use - 100,000 requests/day\n</code></pre> <p>)  rate_limit_per_second: 5.0</p> <p>```  timeout: 30</p> <p>max_retries: 3</p> <p>See the example configuration file for a complete reference.  retry_delay: 1.0</p> <p>For detailed information about each database, see the Database Overview.dblp:</p> <p>enabled: true   # No API key required   # Be respectful with rate limiting   rate_limit_per_second: 2.0   timeout: 30   max_retries: 3   retry_delay: 1.0</p>"},{"location":"contributing/","title":"Contributing to PaperSeek","text":"<p>Thank you for considering contributing to PaperSeek! This document provides guidelines and instructions for contributing.</p>"},{"location":"contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Getting Started</li> <li>Development Setup</li> <li>How to Contribute</li> <li>Code Style</li> <li>Testing</li> <li>Documentation</li> <li>Pull Request Process</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help others learn and grow</li> <li>Keep discussions professional</li> </ul>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/TorSalve/paperseek.git\ncd paperseek\n</code></pre></li> <li>Add upstream remote:    <pre><code>git remote add upstream https://github.com/TorSalve/paperseek.git\n</code></pre></li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install in development mode:    <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> <li> <p>Set up pre-commit hooks (optional):    <pre><code>pip install pre-commit\npre-commit install\n</code></pre></p> </li> </ol>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<ol> <li>Check if the bug has already been reported in Issues</li> <li>If not, create a new issue with:</li> <li>Clear title and description</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Python version and OS</li> <li>Relevant code snippets or error messages</li> </ol>"},{"location":"contributing/#suggesting-features","title":"Suggesting Features","text":"<ol> <li>Check existing Issues and Pull Requests</li> <li>Create a new issue describing:</li> <li>The problem you're trying to solve</li> <li>Your proposed solution</li> <li>Any alternatives you've considered</li> <li>Example usage</li> </ol>"},{"location":"contributing/#adding-new-database-clients","title":"Adding New Database Clients","text":"<p>To add support for a new academic database:</p> <ol> <li> <p>Create a new client file in <code>src/paperseek/clients/</code>:    <pre><code>from ..core.base import DatabaseClient\nfrom ..core.models import Paper, SearchFilters, SearchResult\n\nclass NewDatabaseClient(DatabaseClient):\n    BASE_URL = \"https://api.newdatabase.org\"\n\n    @property\n    def database_name(self) -&gt; str:\n        return \"newdatabase\"\n\n    def search(self, filters: SearchFilters) -&gt; SearchResult:\n        # Implement search logic\n        pass\n\n    def get_by_doi(self, doi: str) -&gt; Optional[Paper]:\n        # Implement DOI lookup\n        pass\n\n    def _normalize_paper(self, raw_data: Dict[str, Any]) -&gt; Paper:\n        # Normalize API response to Paper model\n        pass\n</code></pre></p> </li> <li> <p>Add configuration in <code>core/config.py</code>:    <pre><code>newdatabase: DatabaseConfig = Field(default_factory=DatabaseConfig)\n</code></pre></p> </li> <li> <p>Register the client in <code>core/unified_client.py</code>:    <pre><code>available_clients = {\n    # ... existing clients\n    \"newdatabase\": NewDatabaseClient,\n}\n</code></pre></p> </li> <li> <p>Add tests in <code>tests/test_newdatabase.py</code></p> </li> <li> <p>Update documentation in README.md</p> </li> </ol>"},{"location":"contributing/#improving-documentation","title":"Improving Documentation","text":"<ul> <li>Fix typos or unclear explanations</li> <li>Add examples for common use cases</li> <li>Improve docstrings</li> <li>Update README.md with new features</li> </ul>"},{"location":"contributing/#code-style","title":"Code Style","text":""},{"location":"contributing/#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 with some modifications:</p> <ul> <li>Line length: 100 characters (not 79)</li> <li>Use type hints for all functions</li> <li>Use Google-style docstrings</li> </ul>"},{"location":"contributing/#formatting","title":"Formatting","text":"<p>Format your code with Black: <pre><code>black src/ tests/\n</code></pre></p>"},{"location":"contributing/#linting","title":"Linting","text":"<p>Check your code with Ruff: <pre><code>ruff check src/ tests/\n</code></pre></p>"},{"location":"contributing/#type-checking","title":"Type Checking","text":"<p>Run mypy for type checking: <pre><code>mypy src/\n</code></pre></p>"},{"location":"contributing/#example-docstring","title":"Example Docstring","text":"<pre><code>def search(\n    self,\n    venue: Optional[str] = None,\n    year: Optional[int] = None,\n    max_results: int = 100\n) -&gt; SearchResult:\n    \"\"\"\n    Search for academic papers.\n\n    Args:\n        venue: Conference or journal name\n        year: Publication year\n        max_results: Maximum number of results to return\n\n    Returns:\n        SearchResult object containing matching papers\n\n    Raises:\n        SearchError: If search fails\n        ConfigurationError: If configuration is invalid\n\n    Example:\n        &gt;&gt;&gt; client = UnifiedSearchClient()\n        &gt;&gt;&gt; results = client.search(venue='ICML', year=2023)\n        &gt;&gt;&gt; print(len(results))\n        50\n    \"\"\"\n    # Implementation\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest tests/\n\n# Run specific test file\npytest tests/test_unified_client.py\n\n# Run with coverage\npytest --cov=src/paperseek tests/\n\n# Run with verbose output\npytest -v tests/\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<ol> <li>Create test file in <code>tests/</code> directory</li> <li> <p>Use pytest conventions:    <pre><code>def test_feature_name():\n    # Arrange\n    client = UnifiedSearchClient()\n\n    # Act\n    result = client.search(venue='ICML')\n\n    # Assert\n    assert len(result) &gt; 0\n</code></pre></p> </li> <li> <p>Mock external API calls:    <pre><code>from unittest.mock import Mock, patch\n\ndef test_with_mock():\n    with patch('requests.get') as mock_get:\n        mock_get.return_value.json.return_value = {'data': []}\n        # Test code\n</code></pre></p> </li> <li> <p>Test edge cases:</p> </li> <li>Empty results</li> <li>API errors</li> <li>Invalid input</li> <li>Rate limiting</li> </ol>"},{"location":"contributing/#test-coverage","title":"Test Coverage","text":"<ul> <li>Aim for &gt;80% code coverage</li> <li>Test all public methods</li> <li>Include integration tests</li> <li>Test error handling</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#docstrings","title":"Docstrings","text":"<ul> <li>Add docstrings to all public classes and methods</li> <li>Use Google-style format</li> <li>Include type hints</li> <li>Provide examples where helpful</li> </ul>"},{"location":"contributing/#readme-updates","title":"README Updates","text":"<p>When adding features: 1. Update the feature list 2. Add usage examples 3. Update API documentation 4. Note any breaking changes</p>"},{"location":"contributing/#example-scripts","title":"Example Scripts","text":"<p>Add examples in <code>examples/</code> directory: <pre><code>\"\"\"\nExample: Using the new feature.\n\nThis example demonstrates how to use the newly added feature.\n\"\"\"\n\nfrom paperseek import UnifiedSearchClient\n\ndef main():\n    # Your example code\n    pass\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li> <p>Update from upstream:    <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre></p> </li> <li> <p>Create a feature branch:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes:</p> </li> <li>Write code</li> <li>Add tests</li> <li> <p>Update documentation</p> </li> <li> <p>Run tests and checks:    <pre><code>pytest tests/\nblack src/ tests/\nruff check src/ tests/\nmypy src/\n</code></pre></p> </li> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"Add feature: brief description\"\n</code></pre></p> </li> </ol>"},{"location":"contributing/#commit-message-guidelines","title":"Commit Message Guidelines","text":"<p>Use clear, descriptive commit messages:</p> <pre><code>Add support for PubMed database\n\n- Implement PubMedClient class\n- Add PMID lookup functionality\n- Include tests and documentation\n- Update README with usage examples\n\nCloses #123\n</code></pre> <p>Format: - First line: Summary (50 chars max) - Blank line - Detailed description - Reference related issues</p>"},{"location":"contributing/#submitting-pull-request","title":"Submitting Pull Request","text":"<ol> <li> <p>Push to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create Pull Request on GitHub:</p> </li> <li>Clear title</li> <li>Describe your changes</li> <li>Reference related issues</li> <li> <p>List breaking changes (if any)</p> </li> <li> <p>Pull Request Checklist:</p> </li> <li>[ ] Tests pass locally</li> <li>[ ] Code follows style guidelines</li> <li>[ ] Documentation updated</li> <li>[ ] No merge conflicts</li> <li> <p>[ ] Changes are focused (one feature/fix per PR)</p> </li> <li> <p>Respond to feedback:</p> </li> <li>Address review comments</li> <li>Push updates to your branch</li> <li>Be open to suggestions</li> </ol>"},{"location":"contributing/#after-merge","title":"After Merge","text":"<ol> <li> <p>Delete your branch:    <pre><code>git branch -d feature/your-feature-name\n</code></pre></p> </li> <li> <p>Update your fork:    <pre><code>git checkout main\ngit pull upstream main\ngit push origin main\n</code></pre></p> </li> </ol>"},{"location":"contributing/#areas-for-contribution","title":"Areas for Contribution","text":""},{"location":"contributing/#high-priority","title":"High Priority","text":"<ul> <li>[x] Add support for more databases (PubMed, arXiv, etc.)</li> <li>[x] Improve test coverage</li> <li>[ ] Add CLI tool enhancements</li> <li>[ ] Performance optimizations</li> </ul>"},{"location":"contributing/#medium-priority","title":"Medium Priority","text":"<ul> <li>[ ] Add more export formats (RIS, EndNote)</li> <li>[ ] Implement caching mechanism</li> <li>[ ] Add progress bars for long operations</li> <li>[ ] Improve error messages</li> </ul>"},{"location":"contributing/#low-priority","title":"Low Priority","text":"<ul> <li>[ ] Add visualization tools</li> <li>[ ] Create web interface</li> <li>[ ] Add more examples</li> <li>[ ] Improve documentation</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue</li> <li>Check existing discussions</li> <li>Email: torsalve@di.ku.dk</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"contributing/#thank-you","title":"Thank You!","text":"<p>Thank you for contributing to PaperSeek! Your contributions help make academic research more accessible to everyone.</p> <p>Last Updated: 2025-11-05</p>"},{"location":"databases/","title":"Database Support Documentation","text":"<p>This document provides detailed information about all supported academic databases.</p>"},{"location":"databases/#supported-databases","title":"Supported Databases","text":""},{"location":"databases/#1-crossref","title":"1. CrossRef","text":"<ul> <li>Website: https://www.crossref.org/</li> <li>Description: Comprehensive metadata for scholarly publications from publishers</li> <li>API Key: Optional (not required for basic access)</li> <li>Rate Limits: ~50 requests/second (higher with polite pool)</li> <li>Specialization: General academic publications, DOI metadata</li> <li>Key Features:</li> <li>Extensive DOI coverage</li> <li>Publisher metadata</li> <li>Citation data</li> <li>Funder information</li> </ul>"},{"location":"databases/#2-openalex","title":"2. OpenAlex","text":"<ul> <li>Website: https://openalex.org/</li> <li>Description: Fully open catalog of scholarly papers, authors, and institutions</li> <li>API Key: Optional (Premium tier for enhanced features)</li> <li>Rate Limits: ~10 req/sec with polite pool, higher with Premium</li> <li>Specialization: Open academic data, comprehensive coverage</li> <li>Key Features:</li> <li>Completely open data</li> <li>Author profiles</li> <li>Institution data</li> <li>Citation networks</li> <li>Concepts/topics</li> <li>Open access status</li> </ul>"},{"location":"databases/#3-semantic-scholar","title":"3. Semantic Scholar","text":"<ul> <li>Website: https://www.semanticscholar.org/</li> <li>Description: AI-powered academic search with citation context</li> <li>API Key: Recommended for higher limits</li> <li>Rate Limits: ~1 req/sec (without key), ~10 req/sec (with key)</li> <li>Specialization: Computer science, neuroscience, biomedical</li> <li>Key Features:</li> <li>AI-generated paper summaries</li> <li>Influential citations</li> <li>Citation context</li> <li>Author impact metrics</li> <li>Batch lookup support</li> </ul>"},{"location":"databases/#4-doiorg","title":"4. DOI.org","text":"<ul> <li>Website: https://www.doi.org/</li> <li>Description: DOI resolution service for content negotiation</li> <li>API Key: Not required</li> <li>Rate Limits: ~2 req/sec recommended</li> <li>Specialization: DOI metadata resolution</li> <li>Key Features:</li> <li>Metadata from publishers</li> <li>Content negotiation</li> <li>Multiple format support</li> </ul>"},{"location":"databases/#5-pubmed","title":"5. PubMed","text":"<ul> <li>Website: https://pubmed.ncbi.nlm.nih.gov/</li> <li>Description: NCBI's database of biomedical and life sciences literature</li> <li>API Key: Optional (10 req/sec with key vs 3 req/sec without)</li> <li>Rate Limits: 3 req/sec (without key), 10 req/sec (with key)</li> <li>Specialization: Biomedical, life sciences, clinical research</li> <li>Key Features:</li> <li>36+ million citations</li> <li>MeSH terms (Medical Subject Headings)</li> <li>Clinical trials</li> <li>PubMed Central full-text links</li> <li>Author affiliations</li> <li>Grant information</li> <li>Registration: Get API key at https://www.ncbi.nlm.nih.gov/account/</li> </ul>"},{"location":"databases/#6-arxiv","title":"6. arXiv","text":"<ul> <li>Website: https://arxiv.org/</li> <li>Description: Preprint repository for physics, mathematics, CS, and more</li> <li>API Key: Not required</li> <li>Rate Limits: 1 request per 3 seconds recommended</li> <li>Specialization: Preprints in physics, math, CS, quantitative biology/finance</li> <li>Key Features:</li> <li>Free full-text access</li> <li>Latest research (preprints)</li> <li>LaTeX source available</li> <li>Version history</li> <li>Category classification</li> <li>All papers are open access</li> <li>Note: No DOI-based search; use arXiv ID for direct lookup</li> </ul>"},{"location":"databases/#7-core","title":"7. CORE","text":"<ul> <li>Website: https://core.ac.uk/</li> <li>Description: World's largest collection of open access research papers</li> <li>API Key: Required (free registration)</li> <li>Rate Limits: 10,000 requests/day (free tier)</li> <li>Specialization: Open access papers from repositories and journals</li> <li>Key Features:</li> <li>300+ million papers</li> <li>Full-text search</li> <li>Repository aggregation</li> <li>Download links</li> <li>All papers are open access</li> <li>Registration: Get API key at https://core.ac.uk/services/api</li> </ul>"},{"location":"databases/#8-unpaywall","title":"8. Unpaywall","text":"<ul> <li>Website: https://unpaywall.org/</li> <li>Description: Database of free scholarly articles, finds legal open access versions</li> <li>API Key: Not required (email required)</li> <li>Rate Limits: 100,000 requests/day</li> <li>Specialization: Finding open access versions of paywalled papers</li> <li>Key Features:</li> <li>Legal OA link finding</li> <li>Publisher policies</li> <li>Repository copies</li> <li>Best OA location</li> <li>OA status tracking</li> <li>Note: Primarily DOI-based lookup, not full-text search</li> <li>Best Use: Check if a specific DOI has an open access version</li> </ul>"},{"location":"databases/#9-dblp","title":"9. DBLP","text":"<ul> <li>Website: https://dblp.org/</li> <li>Description: Computer science bibliography</li> <li>API Key: Not required</li> <li>Rate Limits: Be respectful, ~2 req/sec recommended</li> <li>Specialization: Computer science publications</li> <li>Key Features:</li> <li>Comprehensive CS coverage</li> <li>Conference proceedings</li> <li>Journal articles</li> <li>Author pages</li> <li>Venue information</li> <li>Clean bibliographic data</li> <li>Best Use: Computer science papers, especially conferences</li> </ul>"},{"location":"databases/#database-selection-guide","title":"Database Selection Guide","text":""},{"location":"databases/#by-research-domain","title":"By Research Domain","text":""},{"location":"databases/#biomedical-life-sciences","title":"Biomedical &amp; Life Sciences","text":"<ul> <li>Primary: PubMed</li> <li>Secondary: OpenAlex, CORE</li> <li>Open Access: Unpaywall, CORE</li> </ul>"},{"location":"databases/#computer-science","title":"Computer Science","text":"<ul> <li>Primary: DBLP, Semantic Scholar</li> <li>Secondary: arXiv (for preprints), OpenAlex</li> <li>Conferences: DBLP</li> </ul>"},{"location":"databases/#physics-mathematics","title":"Physics &amp; Mathematics","text":"<ul> <li>Primary: arXiv</li> <li>Secondary: OpenAlex, CORE</li> </ul>"},{"location":"databases/#general-academic","title":"General Academic","text":"<ul> <li>Primary: OpenAlex, CrossRef</li> <li>Secondary: Semantic Scholar, CORE</li> </ul>"},{"location":"databases/#by-use-case","title":"By Use Case","text":""},{"location":"databases/#finding-open-access-pdfs","title":"Finding Open Access PDFs","text":"<ol> <li>Unpaywall (DOI-based)</li> <li>CORE</li> <li>arXiv (preprints)</li> <li>OpenAlex (OA status)</li> </ol>"},{"location":"databases/#comprehensive-metadata","title":"Comprehensive Metadata","text":"<ol> <li>OpenAlex</li> <li>CrossRef</li> <li>Semantic Scholar</li> </ol>"},{"location":"databases/#latest-research-preprints","title":"Latest Research / Preprints","text":"<ol> <li>arXiv</li> <li>OpenAlex</li> </ol>"},{"location":"databases/#citation-analysis","title":"Citation Analysis","text":"<ol> <li>Semantic Scholar</li> <li>OpenAlex</li> <li>CrossRef</li> </ol>"},{"location":"databases/#domain-specific-search","title":"Domain-Specific Search","text":"<ul> <li>Biomedical: PubMed \u2192 OpenAlex \u2192 CORE</li> <li>Computer Science: DBLP \u2192 Semantic Scholar \u2192 arXiv</li> <li>Physics/Math: arXiv \u2192 OpenAlex</li> </ul>"},{"location":"databases/#rate-limiting-summary","title":"Rate Limiting Summary","text":"Database Default Limit With API Key Notes CrossRef ~50/sec N/A Higher with polite pool OpenAlex ~10/sec Higher Need polite pool (email) Semantic Scholar ~1/sec ~10/sec API key recommended DOI.org ~2/sec N/A No official limit PubMed 3/sec 10/sec Email recommended arXiv ~0.33/sec N/A 1 per 3 seconds CORE ~2/sec N/A 10k/day limit Unpaywall ~5/sec N/A 100k/day limit DBLP ~2/sec N/A No official limit"},{"location":"databases/#api-key-requirements","title":"API Key Requirements","text":""},{"location":"databases/#required","title":"Required","text":"<ul> <li>CORE: Must register at https://core.ac.uk/services/api</li> </ul>"},{"location":"databases/#recommended","title":"Recommended","text":"<ul> <li>Semantic Scholar: Register at https://www.semanticscholar.org/product/api</li> <li>PubMed: Register at https://www.ncbi.nlm.nih.gov/account/</li> <li>OpenAlex: Premium tier at https://openalex.org/pricing</li> </ul>"},{"location":"databases/#optional","title":"Optional","text":"<ul> <li>CrossRef: Plus service available</li> <li>DOI.org: Not required</li> <li>arXiv: Not available</li> <li>Unpaywall: Not available (email required)</li> <li>DBLP: Not available</li> </ul>"},{"location":"databases/#email-requirements","title":"Email Requirements","text":"<p>Many APIs request an email address for polite pool access or tracking:</p> <ul> <li>Required: Unpaywall</li> <li>Recommended: OpenAlex, PubMed, CrossRef</li> <li>Optional: Others</li> </ul> <p>Configure email in your config: <pre><code>email: your.email@example.com\n</code></pre></p>"},{"location":"databases/#multi-database-search-strategies","title":"Multi-Database Search Strategies","text":""},{"location":"databases/#sequential-mode-default","title":"Sequential Mode (Default)","text":"<p>Try databases in order until one succeeds: <pre><code>client = UnifiedSearchClient(\n    databases=[\"pubmed\", \"openalex\", \"crossref\"],\n    fallback_mode=\"sequential\"\n)\n</code></pre></p>"},{"location":"databases/#parallel-mode-maximum-coverage","title":"Parallel Mode (Maximum Coverage)","text":"<p>Query all databases simultaneously and merge results: <pre><code>client = UnifiedSearchClient(\n    databases=[\"pubmed\", \"arxiv\", \"core\", \"openalex\"],\n    fallback_mode=\"parallel\"\n)\n</code></pre></p>"},{"location":"databases/#first-only-mode-fastest","title":"First-Only Mode (Fastest)","text":"<p>Use only the first enabled database: <pre><code>client = UnifiedSearchClient(\n    databases=[\"dblp\"],\n    fallback_mode=\"first\"\n)\n</code></pre></p>"},{"location":"databases/#field-coverage-by-database","title":"Field Coverage by Database","text":"Field CrossRef OpenAlex Sem. Scholar DOI PubMed arXiv CORE Unpaywall DBLP Title \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Authors \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Abstract \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 Year \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 DOI \u2713 \u2713 \u2713 \u2713 \u2713 ~ \u2713 \u2713 \u2713 Citations \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 Keywords ~ \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 PDF URL ~ \u2713 \u2713 \u2717 \u2717 \u2713 \u2713 \u2713 ~ OA Status ~ \u2713 ~ \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 <p>Legend: \u2713 = Usually available, ~ = Sometimes available, \u2717 = Rarely/never available</p>"},{"location":"databases/#examples","title":"Examples","text":""},{"location":"databases/#biomedical-research","title":"Biomedical Research","text":"<pre><code>from paperseek import UnifiedSearchClient\n\nclient = UnifiedSearchClient(databases=[\"pubmed\", \"openalex\", \"core\"])\nresults = client.search(\n    title=\"cancer immunotherapy\",\n    year_range=(2020, 2024),\n    max_results=50\n)\n</code></pre>"},{"location":"databases/#computer-science_1","title":"Computer Science","text":"<pre><code>client = UnifiedSearchClient(databases=[\"dblp\", \"semantic_scholar\", \"arxiv\"])\nresults = client.search(\n    venue=\"NeurIPS\",\n    year=2023,\n    author=\"Hinton\",\n    max_results=20\n)\n</code></pre>"},{"location":"databases/#finding-open-access-papers","title":"Finding Open Access Papers","text":"<pre><code># First search for papers\nclient = UnifiedSearchClient(databases=[\"openalex\"])\nresults = client.search(title=\"climate change\", max_results=100)\n\n# Then check for OA versions using DOIs\nunpaywall = UnifiedSearchClient(databases=[\"unpaywall\"])\nfor paper in results.papers:\n    if paper.doi:\n        oa_paper = unpaywall.get_by_doi(paper.doi)\n        if oa_paper and oa_paper.is_open_access:\n            print(f\"Open access: {oa_paper.pdf_url}\")\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>This page provides code examples for common use cases.</p>"},{"location":"examples/#basic-searches","title":"Basic Searches","text":""},{"location":"examples/#simple-query-search","title":"Simple Query Search","text":"<pre><code>from paperseek import UnifiedSearchClient\n\nclient = UnifiedSearchClient(databases=[\"crossref\", \"openalex\"])\nresults = client.search(query=\"quantum computing\", max_results=50)\n\nfor paper in results.papers:\n    print(f\"{paper.title} ({paper.year})\")\n</code></pre>"},{"location":"examples/#search-by-venue","title":"Search by Venue","text":"<pre><code># Find papers from a specific journal or conference\nresults = client.search(\n    venue=\"Nature\",\n    year=2023,\n    max_results=100\n)\n</code></pre>"},{"location":"examples/#search-by-author","title":"Search by Author","text":"<pre><code># Find papers by a specific author\nresults = client.search(\n    author=\"Einstein\",\n    year_range=(1900, 1930),\n    max_results=50\n)\n</code></pre>"},{"location":"examples/#search-with-multiple-filters","title":"Search with Multiple Filters","text":"<pre><code>from paperseek import SearchFilters\n\nfilters = SearchFilters(\n    query=\"neural networks\",\n    venue=\"NeurIPS\",\n    year=2023,\n    min_citations=10,\n    max_results=100\n)\n\nresults = client.search_with_filters(filters)\n</code></pre>"},{"location":"examples/#doi-lookups","title":"DOI Lookups","text":""},{"location":"examples/#single-doi-lookup","title":"Single DOI Lookup","text":"<pre><code>doi = \"10.1038/nature12345\"\nresults = client.lookup_dois([doi])\n\nif results.papers:\n    paper = results.papers[0]\n    print(f\"Title: {paper.title}\")\n    print(f\"Authors: {', '.join(a.name for a in paper.authors)}\")\n</code></pre>"},{"location":"examples/#batch-doi-lookup","title":"Batch DOI Lookup","text":"<pre><code>dois = [\n    \"10.1038/nature12345\",\n    \"10.1126/science.abc123\",\n    \"10.1109/access.2023.1234567\"\n]\n\nresults = client.lookup_dois(dois)\nprint(f\"Found {len(results)} papers out of {len(dois)} DOIs\")\n</code></pre>"},{"location":"examples/#field-statistics-and-filtering","title":"Field Statistics and Filtering","text":""},{"location":"examples/#check-field-availability","title":"Check Field Availability","text":"<pre><code># Get statistics on field coverage\nstats = results.get_field_statistics()\n\nprint(\"Field Coverage:\")\nfor field, info in stats.items():\n    print(f\"  {field}: {info['percentage']:.1f}% ({info['count']}/{info['total']})\")\n</code></pre>"},{"location":"examples/#filter-by-required-fields","title":"Filter by Required Fields","text":"<pre><code># Only keep papers with specific fields\nfiltered = results.filter_by_required_fields([\"doi\", \"abstract\", \"citation_count\"])\nprint(f\"Papers with all required fields: {len(filtered)}/{len(results)}\")\n</code></pre>"},{"location":"examples/#export-with-field-statistics","title":"Export with Field Statistics","text":"<pre><code># Export CSV with field coverage report\nresults.to_csv(\n    \"papers.csv\",\n    include_field_stats=True,\n    columns=[\"title\", \"authors\", \"year\", \"doi\", \"abstract\", \"citation_count\"]\n)\n</code></pre>"},{"location":"examples/#export-formats","title":"Export Formats","text":""},{"location":"examples/#csv-export","title":"CSV Export","text":"<pre><code># Basic CSV export\nresults.to_csv(\"papers.csv\")\n\n# Custom columns\nresults.to_csv(\n    \"papers_custom.csv\",\n    columns=[\"title\", \"authors\", \"year\", \"doi\", \"venue\"]\n)\n\n# With field statistics\nresults.to_csv(\n    \"papers_stats.csv\",\n    include_field_stats=True\n)\n</code></pre>"},{"location":"examples/#json-export","title":"JSON Export","text":"<pre><code># Pretty-printed JSON\nresults.to_json(\"papers.json\", indent=2)\n\n# Compact JSON\nresults.to_json(\"papers_compact.json\")\n</code></pre>"},{"location":"examples/#jsonl-export","title":"JSONL Export","text":"<pre><code># JSON Lines format (one object per line)\nresults.to_jsonl(\"papers.jsonl\")\n</code></pre>"},{"location":"examples/#bibtex-export","title":"BibTeX Export","text":"<pre><code># For citation managers (Zotero, Mendeley, etc.)\nresults.to_bibtex(\"papers.bib\")\n</code></pre>"},{"location":"examples/#pdf-downloading","title":"PDF Downloading","text":""},{"location":"examples/#basic-pdf-download","title":"Basic PDF Download","text":"<pre><code>from paperseek import PDFDownloader\n\n# Initialize with conservative settings\ndownloader = PDFDownloader(\n    output_dir=\"papers/pdfs\",\n    delay_seconds=3.0,  # Polite delay\n    max_file_size_mb=50\n)\n\n# Download PDFs\nstats = downloader.download_papers(results.papers)\n\nprint(f\"\u2713 Downloaded: {stats['successful']}\")\nprint(f\"\u2717 Failed: {stats['failed']}\")\nprint(f\"\u2298 Skipped: {stats['skipped']}\")\n</code></pre>"},{"location":"examples/#download-with-custom-organization","title":"Download with Custom Organization","text":"<pre><code># Organize by database\ndownloader = PDFDownloader(\n    output_dir=\"papers/pdfs\",\n    organize_by_database=True\n)\n\nstats = downloader.download_papers(results.papers)\n</code></pre>"},{"location":"examples/#download-with-progress-tracking","title":"Download with Progress Tracking","text":"<pre><code># Download and print progress\nfor i, paper in enumerate(results.papers, 1):\n    print(f\"Downloading {i}/{len(results)}: {paper.title[:50]}...\")\n    success = downloader.download_paper(paper)\n    if success:\n        print(\"  \u2713 Success\")\n    else:\n        print(\"  \u2717 Failed or skipped\")\n\n# Get final statistics\nstats = downloader.get_statistics()\nprint(f\"\\nTotal: {stats['successful']} successful, {stats['failed']} failed\")\n</code></pre>"},{"location":"examples/#selective-pdf-download","title":"Selective PDF Download","text":"<pre><code># Only download papers with DOI and from specific databases\npapers_to_download = [\n    p for p in results.papers\n    if p.doi and p.source_database in [\"arxiv\", \"core\"]\n]\n\ndownloader = PDFDownloader(output_dir=\"selected_pdfs\")\nstats = downloader.download_papers(papers_to_download)\n</code></pre>"},{"location":"examples/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/#multiple-database-search-with-fallback","title":"Multiple Database Search with Fallback","text":"<pre><code># Sequential fallback: try each database until max_results reached\nclient = UnifiedSearchClient(\n    databases=[\"crossref\", \"openalex\", \"semantic_scholar\"],\n    config_dict={\"fallback_mode\": \"sequential\"}\n)\n\nresults = client.search(query=\"machine learning\", max_results=100)\nprint(f\"Databases queried: {', '.join(results.databases_queried)}\")\n</code></pre>"},{"location":"examples/#parallel-search-across-databases","title":"Parallel Search Across Databases","text":"<pre><code># Query all databases simultaneously\nclient = UnifiedSearchClient(\n    databases=[\"crossref\", \"openalex\", \"semantic_scholar\"],\n    config_dict={\"fallback_mode\": \"parallel\"}\n)\n\nresults = client.search(query=\"quantum computing\", max_results=50)\n</code></pre>"},{"location":"examples/#domain-specific-searches","title":"Domain-Specific Searches","text":""},{"location":"examples/#biomedical-literature","title":"Biomedical Literature","text":"<pre><code># Focus on biomedical databases\nclient = UnifiedSearchClient(databases=[\"pubmed\", \"semantic_scholar\"])\n\nresults = client.search(\n    query=\"CRISPR gene editing\",\n    year=2023,\n    max_results=100\n)\n</code></pre>"},{"location":"examples/#computer-science","title":"Computer Science","text":"<pre><code># Focus on CS databases\nclient = UnifiedSearchClient(databases=[\"dblp\", \"arxiv\", \"semantic_scholar\"])\n\nresults = client.search(\n    query=\"transformer models\",\n    venue=\"NeurIPS OR ICML OR ICLR\",\n    year=2023,\n    max_results=100\n)\n</code></pre>"},{"location":"examples/#open-access-only","title":"Open Access Only","text":"<pre><code># Prioritize OA databases\nclient = UnifiedSearchClient(databases=[\"arxiv\", \"core\", \"unpaywall\"])\n\nresults = client.search(\n    query=\"climate change\",\n    max_results=100\n)\n\n# Download PDFs (all should be OA)\ndownloader = PDFDownloader(output_dir=\"oa_papers\")\ndownloader.download_papers(results.papers)\n</code></pre>"},{"location":"examples/#large-scale-data-collection","title":"Large-Scale Data Collection","text":"<pre><code>import time\n\n# Collect data for multiple queries\nqueries = [\n    \"machine learning\",\n    \"deep learning\",\n    \"neural networks\",\n    \"reinforcement learning\"\n]\n\nall_results = []\n\nfor query in queries:\n    print(f\"Searching for: {query}\")\n    results = client.search(query=query, year=2023, max_results=500)\n    all_results.append(results)\n\n    # Export intermediate results\n    results.to_csv(f\"results_{query.replace(' ', '_')}.csv\")\n\n    # Polite delay between queries\n    time.sleep(5)\n\n# Combine and export all results\ncombined = all_results[0]\nfor r in all_results[1:]:\n    combined.extend(r.papers)\n\ncombined.to_csv(\"all_results.csv\", include_field_stats=True)\n</code></pre>"},{"location":"examples/#citation-network-analysis","title":"Citation Network Analysis","text":"<pre><code># Get papers and their citations\nresults = client.search(query=\"graph neural networks\", max_results=100)\n\n# Filter papers with high citation counts\nhighly_cited = [p for p in results.papers if (p.citation_count or 0) &gt; 100]\n\n# Look up citing papers for each\ncitation_network = {}\nfor paper in highly_cited:\n    if paper.doi:\n        citing = client.search(query=f'cites:\"{paper.doi}\"', max_results=50)\n        citation_network[paper.doi] = citing.papers\n\nprint(f\"Built citation network with {len(citation_network)} seed papers\")\n</code></pre>"},{"location":"examples/#command-line-interface","title":"Command Line Interface","text":"<p>The package includes a CLI tool for quick searches:</p> <pre><code># Basic search\npython search_cli.py \"machine learning\" --max-results 50\n\n# Search with filters\npython search_cli.py \"quantum computing\" \\\n    --venue \"Nature\" \\\n    --year 2023 \\\n    --max-results 100\n\n# Export to CSV\npython search_cli.py \"neural networks\" \\\n    --max-results 200 \\\n    --output papers.csv\n\n# Multiple databases\npython search_cli.py \"climate change\" \\\n    --databases crossref openalex semantic_scholar \\\n    --max-results 100\n</code></pre> <p>For more details, see the CLI documentation.</p>"},{"location":"examples/#see-also","title":"See Also","text":"<ul> <li>Database Overview - Learn about each database</li> <li>PDF Downloader - Detailed PDF download guide</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<p>Install the package using pip:</p> <pre><code>pip install paperseek\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>To install for development with all dev dependencies:</p> <pre><code># Clone the repository\ngit clone https://github.com/TorSalve/paperseek.git\ncd paperseek\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev]\"\n</code></pre> <p>Or using requirements files:</p> <pre><code>pip install -r requirements.txt\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"installation/#dependencies","title":"Dependencies","text":""},{"location":"installation/#core-dependencies","title":"Core Dependencies","text":"<p>The package requires the following runtime dependencies:</p> <ul> <li><code>requests&gt;=2.31.0</code> - HTTP client for API requests</li> <li><code>pydantic&gt;=2.0.0</code> - Data validation and models</li> <li><code>pydantic-settings&gt;=2.0.0</code> - Settings management</li> <li><code>pandas&gt;=1.5.0</code> - Data manipulation for exports</li> <li><code>pyyaml&gt;=6.0</code> - YAML configuration support</li> <li><code>pyrate-limiter&gt;=3.0.0</code> - Rate limiting</li> <li><code>python-dotenv&gt;=1.0.0</code> - Environment variable management</li> <li><code>bibtexparser&gt;=1.4.0</code> - BibTeX export support</li> </ul>"},{"location":"installation/#development-dependencies","title":"Development Dependencies","text":"<p>For development, testing, and documentation:</p> <ul> <li><code>pytest&gt;=7.4.0</code> - Testing framework</li> <li><code>pytest-cov&gt;=4.1.0</code> - Coverage reporting</li> <li><code>pytest-mock&gt;=3.11.0</code> - Mocking support</li> <li><code>black&gt;=23.0.0</code> - Code formatting</li> <li><code>mypy&gt;=1.5.0</code> - Type checking</li> <li><code>ruff&gt;=0.0.290</code> - Linting</li> <li><code>mkdocs&gt;=1.5.0</code> - Documentation</li> <li><code>mkdocs-material&gt;=9.4.0</code> - Documentation theme</li> </ul>"},{"location":"installation/#configuration","title":"Configuration","text":"<p>After installation, you may want to set up API keys for certain databases:</p>"},{"location":"installation/#required-api-keys","title":"Required API Keys","text":"<ul> <li>CORE: Requires API key (get one here)</li> <li>Unpaywall: Requires email address (read more)</li> </ul>"},{"location":"installation/#optional-api-keys","title":"Optional API Keys","text":"<p>Some databases work better with API keys but don't require them:</p> <ul> <li>OpenAlex: Works without key, but rate limits are higher with one</li> <li>Semantic Scholar: Works without key, but rate limits are higher with one</li> </ul>"},{"location":"installation/#setting-up-keys","title":"Setting Up Keys","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># Required for CORE\nCORE_API_KEY=your_core_api_key\n\n# Required for Unpaywall\nUNPAYWALL_EMAIL=your.email@example.com\n\n# Optional but recommended\nOPENALEX_EMAIL=your.email@example.com\nSEMANTIC_SCHOLAR_API_KEY=your_s2_api_key\n</code></pre> <p>Or create a <code>config.yaml</code> file:</p> <pre><code>databases:\n  core:\n    enabled: true\n    api_key: \"your_core_api_key\"\n\n  unpaywall:\n    enabled: true\n    email: \"your.email@example.com\"\n\n  openalex:\n    enabled: true\n    email: \"your.email@example.com\"\n</code></pre>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>Test your installation:</p> <pre><code>from paperseek import UnifiedSearchClient\n\nclient = UnifiedSearchClient(databases=[\"crossref\"])\nresults = client.search(query=\"test\", max_results=1)\nprint(f\"Installation successful! Found {len(results)} result(s)\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors, ensure the package is installed:</p> <pre><code>pip list | grep paperseek\n</code></pre>"},{"location":"installation/#api-key-issues","title":"API Key Issues","text":"<p>If you get authentication errors:</p> <ol> <li>Check that your <code>.env</code> file is in the correct location</li> <li>Verify your API keys are valid</li> <li>Check that <code>python-dotenv</code> is installed</li> </ol>"},{"location":"installation/#rate-limiting","title":"Rate Limiting","text":"<p>If you hit rate limits:</p> <ol> <li>Configure rate limiting in your config file</li> <li>Use fewer databases simultaneously</li> <li>Add delays between searches</li> </ol> <p>For more help, see the Contributing Guide or open an issue on GitHub.</p>"},{"location":"pdf_downloader/","title":"PDF Downloader Documentation","text":""},{"location":"pdf_downloader/#overview","title":"Overview","text":"<p>The <code>PDFDownloader</code> utility provides a polite, conservative way to download open access PDFs from academic papers. It includes built-in rate limiting, content verification, and progress tracking to ensure respectful use of academic resources.</p>"},{"location":"pdf_downloader/#key-features","title":"Key Features","text":""},{"location":"pdf_downloader/#polite-conservative","title":"Polite &amp; Conservative","text":"<ul> <li>Rate Limiting: Default 3 seconds between downloads (configurable)</li> <li>Content Verification: Validates downloaded files are actually PDFs</li> <li>File Size Limits: Prevents downloading excessively large files (default: 50 MB)</li> <li>User-Agent Headers: Identifies the downloader properly</li> <li>Email Support: Include email for tracking and polite requests</li> <li>SSL Verification: Respects HTTPS certificates</li> </ul>"},{"location":"pdf_downloader/#smart-download-management","title":"Smart Download Management","text":"<ul> <li>Duplicate Detection: Skip already downloaded files</li> <li>Resume Capability: Won't re-download existing valid PDFs</li> <li>Organize by Database: Automatically organize files by source</li> <li>Progress Tracking: Monitor success rates and statistics</li> <li>Error Handling: Gracefully handles timeouts, HTTP errors, and invalid content</li> </ul>"},{"location":"pdf_downloader/#batch-operations","title":"Batch Operations","text":"<ul> <li>Download from Search Results: Process entire result sets</li> <li>Filter by Open Access: Only download OA papers (respects copyright)</li> <li>Max Download Limits: Set maximum number of files to download</li> <li>Subdirectory Support: Organize downloads into folders</li> </ul>"},{"location":"pdf_downloader/#basic-usage","title":"Basic Usage","text":""},{"location":"pdf_downloader/#simple-download","title":"Simple Download","text":"<pre><code>from paperseek import UnifiedSearchClient, PDFDownloader\n\n# Search for papers\nclient = UnifiedSearchClient(databases=['arxiv'])\nresults = client.search(title='neural networks', max_results=10)\n\n# Download PDFs\nwith PDFDownloader(\n    download_dir='./papers',\n    email='you@example.com'\n) as downloader:\n    downloaded = downloader.download_search_results(results)\n    print(f\"Downloaded {len(downloaded)} PDFs\")\n</code></pre>"},{"location":"pdf_downloader/#download-individual-paper","title":"Download Individual Paper","text":"<pre><code>from paperseek import PDFDownloader\nfrom paperseek.core.models import Paper\n\n# Create downloader\ndownloader = PDFDownloader(\n    download_dir='./downloads',\n    rate_limit_seconds=3.0,\n    email='you@example.com'\n)\n\n# Download a single paper\npaper = Paper(\n    title='Attention Is All You Need',\n    pdf_url='https://arxiv.org/pdf/1706.03762.pdf',\n    doi='10.48550/arXiv.1706.03762'\n)\n\nfilepath = downloader.download_paper(paper)\nif filepath:\n    print(f\"Downloaded to: {filepath}\")\n\ndownloader.close()\n</code></pre>"},{"location":"pdf_downloader/#configuration-options","title":"Configuration Options","text":""},{"location":"pdf_downloader/#constructor-parameters","title":"Constructor Parameters","text":"<pre><code>PDFDownloader(\n    download_dir='./downloads',        # Where to save PDFs\n    rate_limit_seconds=3.0,           # Seconds between downloads\n    timeout=60,                        # Request timeout\n    max_file_size_mb=50,              # Max file size to download\n    user_agent='...',                  # User-Agent string\n    email=None,                        # Email for polite requests\n    overwrite=False,                   # Overwrite existing files\n    verify_ssl=True,                   # Verify SSL certificates\n)\n</code></pre>"},{"location":"pdf_downloader/#recommended-settings-by-use-case","title":"Recommended Settings by Use Case","text":""},{"location":"pdf_downloader/#conservative-default","title":"Conservative (Default)","text":"<p>Best for public use, respects all server limits: <pre><code>downloader = PDFDownloader(\n    rate_limit_seconds=3.0,  # 3 seconds between downloads\n    max_file_size_mb=50,\n    email='you@example.com'\n)\n</code></pre></p>"},{"location":"pdf_downloader/#very-conservative","title":"Very Conservative","text":"<p>For sensitive sources or high-traffic servers: <pre><code>downloader = PDFDownloader(\n    rate_limit_seconds=5.0,  # 5 seconds between downloads\n    max_file_size_mb=30,\n    timeout=90,\n    email='you@example.com'\n)\n</code></pre></p>"},{"location":"pdf_downloader/#moderate","title":"Moderate","text":"<p>When downloading from known-friendly sources: <pre><code>downloader = PDFDownloader(\n    rate_limit_seconds=2.0,  # 2 seconds between downloads\n    max_file_size_mb=100,\n    email='you@example.com'\n)\n</code></pre></p>"},{"location":"pdf_downloader/#methods","title":"Methods","text":""},{"location":"pdf_downloader/#download_paper","title":"download_paper()","text":"<p>Download a single paper's PDF.</p> <pre><code>def download_paper(\n    paper: Paper,\n    filename: Optional[str] = None,\n    subdirectory: Optional[str] = None,\n) -&gt; Optional[Path]\n</code></pre> <p>Parameters: - <code>paper</code>: Paper object with <code>pdf_url</code> attribute - <code>filename</code>: Optional custom filename (auto-generated if not provided) - <code>subdirectory</code>: Optional subdirectory within <code>download_dir</code></p> <p>Returns: Path to downloaded file, or None if failed</p> <p>Example: <pre><code>filepath = downloader.download_paper(\n    paper,\n    filename='custom_name.pdf',\n    subdirectory='arxiv_papers'\n)\n</code></pre></p>"},{"location":"pdf_downloader/#download_papers","title":"download_papers()","text":"<p>Download PDFs for multiple papers.</p> <pre><code>def download_papers(\n    papers: List[Paper],\n    subdirectory: Optional[str] = None,\n    max_downloads: Optional[int] = None,\n) -&gt; Dict[str, Path]\n</code></pre> <p>Parameters: - <code>papers</code>: List of Paper objects - <code>subdirectory</code>: Optional subdirectory - <code>max_downloads</code>: Maximum number to download (for testing/limiting)</p> <p>Returns: Dictionary mapping paper titles to file paths</p> <p>Example: <pre><code>downloaded = downloader.download_papers(\n    papers,\n    subdirectory='ml_papers',\n    max_downloads=5  # Download only first 5\n)\n</code></pre></p>"},{"location":"pdf_downloader/#download_search_results","title":"download_search_results()","text":"<p>Download PDFs from search results with filtering.</p> <pre><code>def download_search_results(\n    search_result: SearchResult,\n    subdirectory: Optional[str] = None,\n    max_downloads: Optional[int] = None,\n    only_open_access: bool = True,\n) -&gt; Dict[str, Path]\n</code></pre> <p>Parameters: - <code>search_result</code>: SearchResult object from client.search() - <code>subdirectory</code>: Optional subdirectory - <code>max_downloads</code>: Maximum number to download - <code>only_open_access</code>: Only download papers marked as OA (default: True)</p> <p>Returns: Dictionary mapping paper titles to file paths</p> <p>Example: <pre><code>downloaded = downloader.download_search_results(\n    results,\n    only_open_access=True,  # Only download OA papers\n    max_downloads=10\n)\n</code></pre></p>"},{"location":"pdf_downloader/#get_statistics-print_statistics","title":"get_statistics() / print_statistics()","text":"<p>Get or print download statistics.</p> <pre><code>stats = downloader.get_statistics()\nprint(f\"Success rate: {stats['success_rate']:.1f}%\")\nprint(f\"Total size: {stats['total_mb']:.2f} MB\")\n\n# Or print formatted statistics\ndownloader.print_statistics()\n</code></pre> <p>Statistics included: - <code>attempted</code>: Number of download attempts - <code>successful</code>: Number of successful downloads - <code>failed</code>: Number of failed downloads - <code>skipped</code>: Number of skipped (already exist) - <code>success_rate</code>: Percentage successful - <code>total_mb</code>: Total data downloaded in MB</p>"},{"location":"pdf_downloader/#file-naming","title":"File Naming","text":"<p>The downloader automatically generates safe filenames based on paper metadata:</p> <ol> <li>DOI-based (preferred): <code>10.48550_arXiv.1706.03762.pdf</code></li> <li>Source ID: <code>arxiv_1706.03762.pdf</code></li> <li>URL hash (fallback): <code>paper_a3b2c1d4e5f6.pdf</code></li> </ol>"},{"location":"pdf_downloader/#custom-filenames","title":"Custom Filenames","text":"<pre><code># Use custom filename\ndownloader.download_paper(paper, filename='my_paper.pdf')\n\n# Or generate based on paper attributes\nfilename = f\"{paper.year}_{paper.authors[0].name.split()[0]}.pdf\"\ndownloader.download_paper(paper, filename=filename)\n</code></pre>"},{"location":"pdf_downloader/#organization-strategies","title":"Organization Strategies","text":""},{"location":"pdf_downloader/#by-database","title":"By Database","text":"<pre><code># Organize by source database\nfor paper in results.papers:\n    if paper.pdf_url:\n        downloader.download_paper(\n            paper,\n            subdirectory=paper.source_database\n        )\n# Creates: downloads/arxiv/paper1.pdf\n#          downloads/core/paper2.pdf\n</code></pre>"},{"location":"pdf_downloader/#by-year","title":"By Year","text":"<pre><code>for paper in results.papers:\n    if paper.pdf_url and paper.year:\n        downloader.download_paper(\n            paper,\n            subdirectory=str(paper.year)\n        )\n</code></pre>"},{"location":"pdf_downloader/#by-topicquery","title":"By Topic/Query","text":"<pre><code># Different topics in different folders\ntopics = {\n    'machine learning': ['neural networks', 'deep learning'],\n    'nlp': ['natural language', 'transformers'],\n}\n\nfor topic, keywords in topics.items():\n    for keyword in keywords:\n        results = client.search(title=keyword, max_results=20)\n        downloader.download_search_results(\n            results,\n            subdirectory=topic,\n            max_downloads=10\n        )\n</code></pre>"},{"location":"pdf_downloader/#error-handling","title":"Error Handling","text":"<p>The downloader handles various error conditions gracefully:</p>"},{"location":"pdf_downloader/#common-issues","title":"Common Issues","text":"<p>File size too large: <pre><code># Increase limit if needed\ndownloader = PDFDownloader(max_file_size_mb=100)\n</code></pre></p> <p>Timeout errors: <pre><code># Increase timeout for slow connections\ndownloader = PDFDownloader(timeout=120)\n</code></pre></p> <p>Invalid PDFs: - Automatically detected via magic number check - HTML responses are rejected - Invalid files are removed</p> <p>SSL certificate errors: <pre><code># Disable SSL verification (not recommended for production)\ndownloader = PDFDownloader(verify_ssl=False)\n</code></pre></p>"},{"location":"pdf_downloader/#logging","title":"Logging","text":"<p>Enable debug logging to see detailed information:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\ndownloader = PDFDownloader(...)\n# Will show detailed download progress\n</code></pre>"},{"location":"pdf_downloader/#best-practices","title":"Best Practices","text":""},{"location":"pdf_downloader/#1-always-use-email","title":"1. Always Use Email","text":"<pre><code># GOOD: Identifies you for tracking\ndownloader = PDFDownloader(email='you@example.com')\n\n# BAD: Anonymous downloading\ndownloader = PDFDownloader()\n</code></pre>"},{"location":"pdf_downloader/#2-respect-rate-limits","title":"2. Respect Rate Limits","text":"<pre><code># GOOD: Conservative rate limiting\ndownloader = PDFDownloader(rate_limit_seconds=3.0)\n\n# BAD: Too aggressive\ndownloader = PDFDownloader(rate_limit_seconds=0.1)\n</code></pre>"},{"location":"pdf_downloader/#3-use-open-access-filtering","title":"3. Use Open Access Filtering","text":"<pre><code># GOOD: Only download OA papers\ndownloaded = downloader.download_search_results(\n    results,\n    only_open_access=True\n)\n\n# CAUTION: May download copyrighted material\ndownloaded = downloader.download_search_results(\n    results,\n    only_open_access=False\n)\n</code></pre>"},{"location":"pdf_downloader/#4-limit-batch-operations","title":"4. Limit Batch Operations","text":"<pre><code># GOOD: Reasonable batch size\ndownloaded = downloader.download_search_results(\n    results,\n    max_downloads=20\n)\n\n# CAUTION: Large batch, takes time\ndownloaded = downloader.download_search_results(\n    results,\n    max_downloads=1000\n)\n</code></pre>"},{"location":"pdf_downloader/#5-use-context-managers","title":"5. Use Context Managers","text":"<pre><code># GOOD: Automatic cleanup\nwith PDFDownloader(...) as downloader:\n    downloaded = downloader.download_search_results(results)\n\n# OK: Manual cleanup\ndownloader = PDFDownloader(...)\ntry:\n    downloaded = downloader.download_search_results(results)\nfinally:\n    downloader.close()\n</code></pre>"},{"location":"pdf_downloader/#complete-examples","title":"Complete Examples","text":""},{"location":"pdf_downloader/#example-1-download-arxiv-papers","title":"Example 1: Download arXiv Papers","text":"<pre><code>from paperseek import UnifiedSearchClient, PDFDownloader\n\n# Search arXiv for recent papers\nclient = UnifiedSearchClient(databases=['arxiv'])\nresults = client.search(\n    title='transformer architecture',\n    year_range=(2023, 2024),\n    max_results=20\n)\n\n# Download with conservative settings\nwith PDFDownloader(\n    download_dir='./arxiv_papers',\n    rate_limit_seconds=3.0,\n    email='researcher@university.edu'\n) as downloader:\n\n    downloaded = downloader.download_search_results(\n        results,\n        max_downloads=10\n    )\n\n    print(f\"\\nDownloaded {len(downloaded)} papers:\")\n    for title, path in downloaded.items():\n        print(f\"  {path.name}\")\n\n    downloader.print_statistics()\n\nclient.close()\n</code></pre>"},{"location":"pdf_downloader/#example-2-find-oa-versions-with-unpaywall","title":"Example 2: Find OA Versions with Unpaywall","text":"<pre><code># Search for papers\nclient = UnifiedSearchClient(databases=['openalex'])\nresults = client.search(\n    title='climate change',\n    year=2023,\n    max_results=50\n)\n\n# Check for OA versions\nunpaywall = UnifiedSearchClient(databases=['unpaywall'])\noa_papers = []\n\nfor paper in results.papers:\n    if paper.doi:\n        oa_version = unpaywall.get_by_doi(paper.doi)\n        if oa_version and oa_version.is_open_access and oa_version.pdf_url:\n            oa_papers.append(oa_version)\n\nprint(f\"Found {len(oa_papers)} papers with OA PDFs\")\n\n# Download OA versions\nwith PDFDownloader(\n    download_dir='./oa_papers',\n    rate_limit_seconds=2.0,\n    email='researcher@university.edu'\n) as downloader:\n\n    downloaded = downloader.download_papers(oa_papers, max_downloads=20)\n    downloader.print_statistics()\n</code></pre>"},{"location":"pdf_downloader/#example-3-organize-by-database","title":"Example 3: Organize by Database","text":"<pre><code># Search multiple databases\nclient = UnifiedSearchClient(\n    databases=['arxiv', 'core', 'openalex'],\n    fallback_mode='parallel'\n)\n\nresults = client.search(\n    title='deep learning',\n    year_range=(2022, 2023),\n    max_results=30\n)\n\n# Download and organize by source\nwith PDFDownloader(\n    download_dir='./papers',\n    rate_limit_seconds=3.0,\n    email='researcher@university.edu'\n) as downloader:\n\n    # Group by database\n    by_db = {}\n    for paper in results.papers:\n        if paper.pdf_url:\n            db = paper.source_database\n            if db not in by_db:\n                by_db[db] = []\n            by_db[db].append(paper)\n\n    # Download each database's papers\n    for db, papers in by_db.items():\n        print(f\"\\nDownloading {len(papers)} papers from {db}...\")\n        downloaded = downloader.download_papers(\n            papers,\n            subdirectory=db,\n            max_downloads=5\n        )\n        print(f\"Downloaded {len(downloaded)} from {db}\")\n\n    downloader.print_statistics()\n</code></pre>"},{"location":"pdf_downloader/#troubleshooting","title":"Troubleshooting","text":""},{"location":"pdf_downloader/#no-pdfs-downloaded","title":"No PDFs Downloaded","text":"<p>Check if papers have PDF URLs: <pre><code>papers_with_pdfs = [p for p in results.papers if p.pdf_url]\nprint(f\"{len(papers_with_pdfs)} papers have PDF URLs\")\n</code></pre></p> <p>Check open access filter: <pre><code># Try without OA filter\ndownloaded = downloader.download_search_results(\n    results,\n    only_open_access=False  # Download all with PDF URLs\n)\n</code></pre></p>"},{"location":"pdf_downloader/#downloads-failing","title":"Downloads Failing","text":"<p>Enable debug logging: <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> <p>Check statistics for patterns: <pre><code>stats = downloader.get_statistics()\nif stats['failed'] &gt; stats['successful']:\n    print(\"Many failures - check network or URLs\")\n</code></pre></p> <p>Test with single paper: <pre><code># Test download capability\npaper = results.papers[0]\nfilepath = downloader.download_paper(paper)\nif not filepath:\n    print(f\"Failed to download: {paper.pdf_url}\")\n</code></pre></p>"},{"location":"pdf_downloader/#slow-downloads","title":"Slow Downloads","text":"<p>Reduce rate limiting (carefully): <pre><code># Only if you're sure the source can handle it\ndownloader = PDFDownloader(rate_limit_seconds=2.0)\n</code></pre></p> <p>Download fewer papers: <pre><code>downloaded = downloader.download_search_results(\n    results,\n    max_downloads=5  # Start small\n)\n</code></pre></p>"},{"location":"pdf_downloader/#rate-limiting-guidelines","title":"Rate Limiting Guidelines","text":""},{"location":"pdf_downloader/#by-source","title":"By Source","text":"Source Recommended Rate Limit Notes arXiv 3.0 seconds As per arXiv guidelines CORE 2.0 seconds Respectful for free tier Unpaywall 2.0 seconds 100k/day limit OpenAlex 3.0 seconds Conservative for OA links PubMed Central 4.0 seconds Very conservative Unknown/Mixed 3.0 seconds Safe default"},{"location":"pdf_downloader/#general-guidelines","title":"General Guidelines","text":"<ul> <li>Default: Use 3.0 seconds for mixed sources</li> <li>Single trusted source: Can use 2.0 seconds</li> <li>High-traffic times: Increase to 5.0 seconds</li> <li>Personal research: 3.0 seconds is respectful</li> <li>Large batches: Consider 4-5 seconds</li> </ul>"},{"location":"pdf_downloader/#legal-ethical-considerations","title":"Legal &amp; Ethical Considerations","text":""},{"location":"pdf_downloader/#copyright","title":"Copyright","text":"<ul> <li>Only download open access papers by default</li> <li>Respect copyright and licensing terms</li> <li>Use <code>only_open_access=True</code> filter</li> <li>Check individual paper licenses</li> </ul>"},{"location":"pdf_downloader/#server-load","title":"Server Load","text":"<ul> <li>Use conservative rate limiting</li> <li>Don't download during peak hours if possible</li> <li>Limit batch sizes for large operations</li> <li>Include email for tracking</li> </ul>"},{"location":"pdf_downloader/#terms-of-service","title":"Terms of Service","text":"<p>Different sources have different terms:</p> <ul> <li>arXiv: Free to download, but be respectful</li> <li>CORE: Free for non-commercial, respect rate limits</li> <li>Unpaywall: 100k requests/day, email required</li> <li>PMC: Free, but follow NCBI guidelines</li> </ul> <p>Always check the specific terms of service for each source.</p>"},{"location":"pdf_downloader/#summary","title":"Summary","text":"<p>The PDF Downloader provides a responsible way to download open access academic papers:</p> <p>\u2705 Conservative by default (3 second delays) \u2705 Respects copyright (OA filtering) \u2705 Verifies content (checks PDF format) \u2705 Tracks progress (statistics and logging) \u2705 Handles errors (timeouts, invalid files) \u2705 Organizes files (by database, year, etc.)  </p> <p>Use it responsibly and respect server resources!</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get started with Academic Search Unified in just a few minutes!</p>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"quickstart/#1-import-and-initialize","title":"1. Import and Initialize","text":"<pre><code>from paperseek import UnifiedSearchClient\n\n# Create a client with default databases\nclient = UnifiedSearchClient(databases=[\"crossref\", \"openalex\"])\n</code></pre>"},{"location":"quickstart/#2-perform-a-search","title":"2. Perform a Search","text":"<pre><code># Simple query search\nresults = client.search(\n    query=\"machine learning\",\n    max_results=50\n)\n\nprint(f\"Found {len(results)} papers\")\n</code></pre>"},{"location":"quickstart/#3-access-results","title":"3. Access Results","text":"<pre><code># Iterate through results\nfor paper in results.papers[:5]:\n    print(f\"Title: {paper.title}\")\n    print(f\"Authors: {', '.join(a.name for a in paper.authors)}\")\n    print(f\"Year: {paper.year}\")\n    print(f\"DOI: {paper.doi}\")\n    print(f\"Citations: {paper.citation_count}\")\n    print()\n</code></pre>"},{"location":"quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"quickstart/#search-by-venue-and-year","title":"Search by Venue and Year","text":"<pre><code># Find papers from a specific conference/journal\nresults = client.search(\n    venue=\"Nature\",\n    year=2023,\n    max_results=100\n)\n</code></pre>"},{"location":"quickstart/#search-with-filters","title":"Search with Filters","text":"<pre><code>from paperseek import SearchFilters\n\n# Create filter object\nfilters = SearchFilters(\n    query=\"neural networks\",\n    year=2023,\n    venue=\"NeurIPS\",\n    min_citations=10,\n    max_results=100\n)\n\nresults = client.search_with_filters(filters)\n</code></pre>"},{"location":"quickstart/#look-up-by-doi","title":"Look Up by DOI","text":"<pre><code># Look up specific papers by DOI\ndois = [\n    \"10.1038/nature12345\",\n    \"10.1126/science.abc123\"\n]\n\nresults = client.lookup_dois(dois)\n</code></pre>"},{"location":"quickstart/#search-by-author","title":"Search by Author","text":"<pre><code># Find papers by author name\nresults = client.search(\n    author=\"Geoffrey Hinton\",\n    year=2023,\n    max_results=50\n)\n</code></pre>"},{"location":"quickstart/#export-results","title":"Export Results","text":""},{"location":"quickstart/#export-to-csv","title":"Export to CSV","text":"<pre><code># Export with default columns\nresults.to_csv(\"papers.csv\")\n\n# Export with custom columns\nresults.to_csv(\n    \"papers.csv\",\n    columns=[\"title\", \"authors\", \"year\", \"doi\", \"citation_count\"]\n)\n</code></pre>"},{"location":"quickstart/#export-to-json","title":"Export to JSON","text":"<pre><code># Pretty-printed JSON\nresults.to_json(\"papers.json\", indent=2)\n\n# Compact JSON Lines format\nresults.to_jsonl(\"papers.jsonl\")\n</code></pre>"},{"location":"quickstart/#export-to-bibtex","title":"Export to BibTeX","text":"<pre><code># For citation managers\nresults.to_bibtex(\"papers.bib\")\n</code></pre>"},{"location":"quickstart/#field-statistics","title":"Field Statistics","text":""},{"location":"quickstart/#check-field-availability","title":"Check Field Availability","text":"<pre><code># See what fields are available\nstats = results.get_field_statistics()\n\nprint(\"Field Coverage:\")\nfor field, coverage in stats.items():\n    print(f\"  {field}: {coverage['percentage']:.1f}%\")\n</code></pre>"},{"location":"quickstart/#filter-by-required-fields","title":"Filter by Required Fields","text":"<pre><code># Only keep papers with DOI and abstract\nfiltered = results.filter_by_required_fields([\"doi\", \"abstract\"])\nprint(f\"Papers with DOI and abstract: {len(filtered)}\")\n</code></pre>"},{"location":"quickstart/#download-pdfs","title":"Download PDFs","text":"<pre><code>from paperseek import PDFDownloader\n\n# Initialize downloader (conservative by default)\ndownloader = PDFDownloader(\n    output_dir=\"papers/pdfs\",\n    delay_seconds=3.0  # Polite delay between downloads\n)\n\n# Download PDFs for papers that have OA links\nstats = downloader.download_papers(results.papers)\n\nprint(f\"Downloaded: {stats['successful']}\")\nprint(f\"Failed: {stats['failed']}\")\nprint(f\"Skipped: {stats['skipped']}\")\n</code></pre>"},{"location":"quickstart/#configuration","title":"Configuration","text":""},{"location":"quickstart/#using-config-file","title":"Using Config File","text":"<p>Create <code>config.yaml</code>:</p> <pre><code>databases:\n  crossref:\n    enabled: true\n    rate_limit_per_second: 1.0\n\n  openalex:\n    enabled: true\n    email: \"your.email@example.com\"\n\n  semantic_scholar:\n    enabled: true\n    api_key: \"your_api_key\"\n\nfallback_mode: \"sequential\"\ndefault_max_results: 100\n</code></pre> <p>Load configuration:</p> <pre><code>client = UnifiedSearchClient(config_path=\"config.yaml\")\n</code></pre>"},{"location":"quickstart/#using-environment-variables","title":"Using Environment Variables","text":"<p>Create <code>.env</code>:</p> <pre><code>OPENALEX_EMAIL=your.email@example.com\nSEMANTIC_SCHOLAR_API_KEY=your_api_key\nCORE_API_KEY=your_core_key\n</code></pre> <p>The client will automatically load these values.</p>"},{"location":"quickstart/#using-config-dictionary","title":"Using Config Dictionary","text":"<pre><code>config = {\n    \"crossref\": {\n        \"enabled\": True,\n        \"rate_limit_per_second\": 1.0\n    },\n    \"openalex\": {\n        \"enabled\": True,\n        \"email\": \"your.email@example.com\"\n    }\n}\n\nclient = UnifiedSearchClient(\n    databases=[\"crossref\", \"openalex\"],\n    config_dict=config\n)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about each database</li> <li>Explore more examples</li> <li>Read the API reference</li> <li>Set up PDF downloading</li> </ul>"},{"location":"api/database_clients/","title":"Database Clients","text":"<p>Individual database client implementations.</p>"},{"location":"api/database_clients/#base-client","title":"Base Client","text":""},{"location":"api/database_clients/#paperseek.core.base.DatabaseClient","title":"DatabaseClient","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for database-specific clients.</p> <p>All database clients must implement the abstract methods.</p>"},{"location":"api/database_clients/#paperseek.core.base.DatabaseClient-functions","title":"Functions","text":""},{"location":"api/database_clients/#paperseek.core.base.DatabaseClient.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search the database with given filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>Search filters</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p>"},{"location":"api/database_clients/#crossref-client","title":"CrossRef Client","text":""},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient","title":"CrossRefClient","text":"<p>               Bases: <code>DatabaseClient</code></p> <p>Client for CrossRef API.</p> <p>CrossRef provides metadata for scholarly works with DOIs. API Documentation: https://api.crossref.org</p>"},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient-attributes","title":"Attributes","text":""},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient.database_name","title":"database_name  <code>property</code>","text":"<pre><code>database_name: str\n</code></pre> <p>Return database name.</p>"},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient-functions","title":"Functions","text":""},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient.__init__","title":"__init__","text":"<pre><code>__init__(\n    config: DatabaseConfig,\n    email: Optional[str] = None,\n    user_agent: str = \"AcademicSearchUnified/0.1.0\",\n)\n</code></pre> <p>Initialize CrossRef client.</p>"},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient.search","title":"search","text":"<pre><code>search(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search CrossRef database.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>Search filters</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p>"},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient.get_by_doi","title":"get_by_doi","text":"<pre><code>get_by_doi(doi: str) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by DOI.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>Digital Object Identifier</p> required <p>Returns:</p> Type Description <code>Optional[Paper]</code> <p>Paper object or None</p>"},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient.get_by_identifier","title":"get_by_identifier","text":"<pre><code>get_by_identifier(\n    identifier: str, id_type: str\n) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by identifier.</p>"},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient.batch_lookup","title":"batch_lookup","text":"<pre><code>batch_lookup(\n    identifiers: List[str], id_type: str\n) -&gt; SearchResult\n</code></pre> <p>Look up multiple papers.</p>"},{"location":"api/database_clients/#paperseek.clients.crossref.CrossRefClient.get_supported_fields","title":"get_supported_fields","text":"<pre><code>get_supported_fields() -&gt; List[str]\n</code></pre> <p>Get fields typically provided by CrossRef.</p>"},{"location":"api/database_clients/#openalex-client","title":"OpenAlex Client","text":""},{"location":"api/database_clients/#paperseek.clients.openalex.OpenAlexClient","title":"OpenAlexClient","text":"<p>               Bases: <code>DatabaseClient</code></p> <p>Client for OpenAlex API.</p> <p>OpenAlex is a fully open catalog of scholarly papers, authors, and institutions. API Documentation: https://docs.openalex.org</p> <p>Polite Pool: - Add email via mailto parameter or User-Agent header for better rate limits (~10 req/sec) - Premium users can use API key (passed as api_key parameter) for higher limits and special filters</p>"},{"location":"api/database_clients/#paperseek.clients.openalex.OpenAlexClient-attributes","title":"Attributes","text":""},{"location":"api/database_clients/#paperseek.clients.openalex.OpenAlexClient.database_name","title":"database_name  <code>property</code>","text":"<pre><code>database_name: str\n</code></pre> <p>Return database name.</p>"},{"location":"api/database_clients/#paperseek.clients.openalex.OpenAlexClient-functions","title":"Functions","text":""},{"location":"api/database_clients/#paperseek.clients.openalex.OpenAlexClient.search","title":"search","text":"<pre><code>search(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search OpenAlex database.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>Search filters</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p>"},{"location":"api/database_clients/#paperseek.clients.openalex.OpenAlexClient.get_by_doi","title":"get_by_doi","text":"<pre><code>get_by_doi(doi: str) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by DOI.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>Digital Object Identifier</p> required <p>Returns:</p> Type Description <code>Optional[Paper]</code> <p>Paper object or None</p>"},{"location":"api/database_clients/#paperseek.clients.openalex.OpenAlexClient.get_by_identifier","title":"get_by_identifier","text":"<pre><code>get_by_identifier(\n    identifier: str, id_type: str\n) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by identifier.</p>"},{"location":"api/database_clients/#paperseek.clients.openalex.OpenAlexClient.batch_lookup","title":"batch_lookup","text":"<pre><code>batch_lookup(\n    identifiers: List[str], id_type: str\n) -&gt; SearchResult\n</code></pre> <p>Look up multiple papers.</p>"},{"location":"api/database_clients/#paperseek.clients.openalex.OpenAlexClient.get_supported_fields","title":"get_supported_fields","text":"<pre><code>get_supported_fields() -&gt; List[str]\n</code></pre> <p>Get fields typically provided by OpenAlex.</p>"},{"location":"api/database_clients/#semantic-scholar-client","title":"Semantic Scholar Client","text":""},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient","title":"SemanticScholarClient","text":"<p>               Bases: <code>DatabaseClient</code></p> <p>Client for Semantic Scholar API.</p> <p>Semantic Scholar is a free, AI-powered research tool for scientific literature. API Documentation: https://api.semanticscholar.org</p>"},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient-attributes","title":"Attributes","text":""},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient.database_name","title":"database_name  <code>property</code>","text":"<pre><code>database_name: str\n</code></pre> <p>Return database name.</p>"},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient-functions","title":"Functions","text":""},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient.__init__","title":"__init__","text":"<pre><code>__init__(\n    config: DatabaseConfig,\n    email: Optional[str] = None,\n    user_agent: str = \"AcademicSearchUnified/0.1.0\",\n)\n</code></pre> <p>Initialize Semantic Scholar client.</p>"},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient.search","title":"search","text":"<pre><code>search(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search Semantic Scholar database.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>Search filters</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p>"},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient.get_by_doi","title":"get_by_doi","text":"<pre><code>get_by_doi(doi: str) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by DOI.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>Digital Object Identifier</p> required <p>Returns:</p> Type Description <code>Optional[Paper]</code> <p>Paper object or None</p>"},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient.get_by_identifier","title":"get_by_identifier","text":"<pre><code>get_by_identifier(\n    identifier: str, id_type: str\n) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by identifier.</p>"},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient.batch_lookup","title":"batch_lookup","text":"<pre><code>batch_lookup(\n    identifiers: List[str], id_type: str\n) -&gt; SearchResult\n</code></pre> <p>Look up multiple papers (supports batch API).</p> <p>Parameters:</p> Name Type Description Default <code>identifiers</code> <code>List[str]</code> <p>List of identifiers</p> required <code>id_type</code> <code>str</code> <p>Type of identifier</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult with found papers</p>"},{"location":"api/database_clients/#paperseek.clients.semantic_scholar.SemanticScholarClient.get_supported_fields","title":"get_supported_fields","text":"<pre><code>get_supported_fields() -&gt; List[str]\n</code></pre> <p>Get fields typically provided by Semantic Scholar.</p>"},{"location":"api/database_clients/#pubmed-client","title":"PubMed Client","text":""},{"location":"api/database_clients/#paperseek.clients.pubmed.PubMedClient","title":"PubMedClient","text":"<p>               Bases: <code>DatabaseClient</code></p> <p>Client for PubMed E-utilities API.</p> <p>PubMed comprises more than 36 million citations for biomedical literature from MEDLINE, life science journals, and online books.</p> <p>API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25501/</p> <p>Note: NCBI requests users provide an email address and API key for better rate limits. - Without API key: 3 requests/second - With API key: 10 requests/second</p>"},{"location":"api/database_clients/#paperseek.clients.pubmed.PubMedClient-attributes","title":"Attributes","text":""},{"location":"api/database_clients/#paperseek.clients.pubmed.PubMedClient.database_name","title":"database_name  <code>property</code>","text":"<pre><code>database_name: str\n</code></pre> <p>Return database name.</p>"},{"location":"api/database_clients/#paperseek.clients.pubmed.PubMedClient-functions","title":"Functions","text":""},{"location":"api/database_clients/#paperseek.clients.pubmed.PubMedClient.search","title":"search","text":"<pre><code>search(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search PubMed database.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>Search filters</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p>"},{"location":"api/database_clients/#paperseek.clients.pubmed.PubMedClient.get_by_doi","title":"get_by_doi","text":"<pre><code>get_by_doi(doi: str) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by DOI.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>Digital Object Identifier</p> required <p>Returns:</p> Type Description <code>Optional[Paper]</code> <p>Paper object or None</p>"},{"location":"api/database_clients/#paperseek.clients.pubmed.PubMedClient.get_by_identifier","title":"get_by_identifier","text":"<pre><code>get_by_identifier(\n    identifier: str, id_type: str\n) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by identifier.</p>"},{"location":"api/database_clients/#paperseek.clients.pubmed.PubMedClient.batch_lookup","title":"batch_lookup","text":"<pre><code>batch_lookup(\n    identifiers: List[str], id_type: str\n) -&gt; SearchResult\n</code></pre> <p>Look up multiple papers.</p>"},{"location":"api/database_clients/#paperseek.clients.pubmed.PubMedClient.get_supported_fields","title":"get_supported_fields","text":"<pre><code>get_supported_fields() -&gt; List[str]\n</code></pre> <p>Get fields typically provided by PubMed.</p>"},{"location":"api/database_clients/#arxiv-client","title":"arXiv Client","text":""},{"location":"api/database_clients/#paperseek.clients.arxiv.ArXivClient","title":"ArXivClient","text":"<p>               Bases: <code>DatabaseClient</code></p> <p>Client for arXiv API.</p> <p>arXiv is a free distribution service and an open-access archive for scholarly articles in physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.</p> <p>API Documentation: https://info.arxiv.org/help/api/index.html</p> <p>Note: No API key required. Rate limiting: 1 request per 3 seconds recommended.</p>"},{"location":"api/database_clients/#paperseek.clients.arxiv.ArXivClient-attributes","title":"Attributes","text":""},{"location":"api/database_clients/#paperseek.clients.arxiv.ArXivClient.database_name","title":"database_name  <code>property</code>","text":"<pre><code>database_name: str\n</code></pre> <p>Return database name.</p>"},{"location":"api/database_clients/#paperseek.clients.arxiv.ArXivClient-functions","title":"Functions","text":""},{"location":"api/database_clients/#paperseek.clients.arxiv.ArXivClient.search","title":"search","text":"<pre><code>search(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search arXiv database.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>Search filters</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p>"},{"location":"api/database_clients/#paperseek.clients.arxiv.ArXivClient.get_by_doi","title":"get_by_doi","text":"<pre><code>get_by_doi(doi: str) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by DOI.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>Digital Object Identifier</p> required <p>Returns:</p> Type Description <code>Optional[Paper]</code> <p>Paper object or None</p>"},{"location":"api/database_clients/#paperseek.clients.arxiv.ArXivClient.get_by_identifier","title":"get_by_identifier","text":"<pre><code>get_by_identifier(\n    identifier: str, id_type: str\n) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by identifier.</p>"},{"location":"api/database_clients/#paperseek.clients.arxiv.ArXivClient.batch_lookup","title":"batch_lookup","text":"<pre><code>batch_lookup(\n    identifiers: List[str], id_type: str\n) -&gt; SearchResult\n</code></pre> <p>Look up multiple papers.</p>"},{"location":"api/database_clients/#paperseek.clients.arxiv.ArXivClient.get_supported_fields","title":"get_supported_fields","text":"<pre><code>get_supported_fields() -&gt; List[str]\n</code></pre> <p>Get fields typically provided by arXiv.</p>"},{"location":"api/database_clients/#core-client","title":"CORE Client","text":""},{"location":"api/database_clients/#paperseek.clients.core.COREClient","title":"COREClient","text":"<p>               Bases: <code>DatabaseClient</code></p> <p>Client for CORE API v3.</p> <p>CORE is the world's largest collection of open access research papers, aggregating millions of articles from repositories and journals worldwide.</p> <p>API Documentation: https://core.ac.uk/documentation/api</p> <p>Note: API key required. Free tier: 10,000 requests/day. Register at: https://core.ac.uk/services/api</p>"},{"location":"api/database_clients/#paperseek.clients.core.COREClient-attributes","title":"Attributes","text":""},{"location":"api/database_clients/#paperseek.clients.core.COREClient.database_name","title":"database_name  <code>property</code>","text":"<pre><code>database_name: str\n</code></pre> <p>Return database name.</p>"},{"location":"api/database_clients/#paperseek.clients.core.COREClient-functions","title":"Functions","text":""},{"location":"api/database_clients/#paperseek.clients.core.COREClient.search","title":"search","text":"<pre><code>search(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search CORE database.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>Search filters</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p>"},{"location":"api/database_clients/#paperseek.clients.core.COREClient.get_by_doi","title":"get_by_doi","text":"<pre><code>get_by_doi(doi: str) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by DOI.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>Digital Object Identifier</p> required <p>Returns:</p> Type Description <code>Optional[Paper]</code> <p>Paper object or None</p>"},{"location":"api/database_clients/#paperseek.clients.core.COREClient.get_by_identifier","title":"get_by_identifier","text":"<pre><code>get_by_identifier(\n    identifier: str, id_type: str\n) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by identifier.</p>"},{"location":"api/database_clients/#paperseek.clients.core.COREClient.batch_lookup","title":"batch_lookup","text":"<pre><code>batch_lookup(\n    identifiers: List[str], id_type: str\n) -&gt; SearchResult\n</code></pre> <p>Look up multiple papers.</p>"},{"location":"api/database_clients/#paperseek.clients.core.COREClient.get_supported_fields","title":"get_supported_fields","text":"<pre><code>get_supported_fields() -&gt; List[str]\n</code></pre> <p>Get fields typically provided by CORE.</p>"},{"location":"api/database_clients/#unpaywall-client","title":"Unpaywall Client","text":""},{"location":"api/database_clients/#paperseek.clients.unpaywall.UnpaywallClient","title":"UnpaywallClient","text":"<p>               Bases: <code>DatabaseClient</code></p> <p>Client for Unpaywall API.</p> <p>Unpaywall is a database of free scholarly articles. It harvests Open Access content from over 50,000 publishers and repositories, and makes it easy to find, track, and use.</p> <p>API Documentation: https://unpaywall.org/products/api</p> <p>Note: Email address required. No API key needed. Free for non-commercial use. Rate limit: 100,000 requests per day.</p>"},{"location":"api/database_clients/#paperseek.clients.unpaywall.UnpaywallClient-attributes","title":"Attributes","text":""},{"location":"api/database_clients/#paperseek.clients.unpaywall.UnpaywallClient.database_name","title":"database_name  <code>property</code>","text":"<pre><code>database_name: str\n</code></pre> <p>Return database name.</p>"},{"location":"api/database_clients/#paperseek.clients.unpaywall.UnpaywallClient-functions","title":"Functions","text":""},{"location":"api/database_clients/#paperseek.clients.unpaywall.UnpaywallClient.search","title":"search","text":"<pre><code>search(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search Unpaywall database.</p> <p>Note: Unpaywall primarily supports DOI lookup, not full-text search. This method will only work if a DOI is provided in filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>Search filters</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p>"},{"location":"api/database_clients/#paperseek.clients.unpaywall.UnpaywallClient.get_by_doi","title":"get_by_doi","text":"<pre><code>get_by_doi(doi: str) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by DOI.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>Digital Object Identifier</p> required <p>Returns:</p> Type Description <code>Optional[Paper]</code> <p>Paper object or None</p>"},{"location":"api/database_clients/#paperseek.clients.unpaywall.UnpaywallClient.get_by_identifier","title":"get_by_identifier","text":"<pre><code>get_by_identifier(\n    identifier: str, id_type: str\n) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by identifier.</p>"},{"location":"api/database_clients/#paperseek.clients.unpaywall.UnpaywallClient.batch_lookup","title":"batch_lookup","text":"<pre><code>batch_lookup(\n    identifiers: List[str], id_type: str\n) -&gt; SearchResult\n</code></pre> <p>Look up multiple papers.</p>"},{"location":"api/database_clients/#paperseek.clients.unpaywall.UnpaywallClient.get_supported_fields","title":"get_supported_fields","text":"<pre><code>get_supported_fields() -&gt; List[str]\n</code></pre> <p>Get fields typically provided by Unpaywall.</p>"},{"location":"api/database_clients/#dblp-client","title":"DBLP Client","text":""},{"location":"api/database_clients/#paperseek.clients.dblp.DBLPClient","title":"DBLPClient","text":"<p>               Bases: <code>DatabaseClient</code></p> <p>Client for DBLP (Computer Science Bibliography) API.</p> <p>DBLP is a comprehensive computer science bibliography providing bibliographic information on major computer science journals and proceedings.</p> <p>API Documentation: https://dblp.org/faq/How+to+use+the+dblp+search+API.html</p> <p>Note: No API key required. Free to use. Rate limiting: Be respectful, no official limit but throttle recommended.</p>"},{"location":"api/database_clients/#paperseek.clients.dblp.DBLPClient-attributes","title":"Attributes","text":""},{"location":"api/database_clients/#paperseek.clients.dblp.DBLPClient.database_name","title":"database_name  <code>property</code>","text":"<pre><code>database_name: str\n</code></pre> <p>Return database name.</p>"},{"location":"api/database_clients/#paperseek.clients.dblp.DBLPClient-functions","title":"Functions","text":""},{"location":"api/database_clients/#paperseek.clients.dblp.DBLPClient.search","title":"search","text":"<pre><code>search(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search DBLP database.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>Search filters</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p>"},{"location":"api/database_clients/#paperseek.clients.dblp.DBLPClient.get_by_doi","title":"get_by_doi","text":"<pre><code>get_by_doi(doi: str) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by DOI.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>Digital Object Identifier</p> required <p>Returns:</p> Type Description <code>Optional[Paper]</code> <p>Paper object or None</p>"},{"location":"api/database_clients/#paperseek.clients.dblp.DBLPClient.get_by_identifier","title":"get_by_identifier","text":"<pre><code>get_by_identifier(\n    identifier: str, id_type: str\n) -&gt; Optional[Paper]\n</code></pre> <p>Get paper by identifier.</p>"},{"location":"api/database_clients/#paperseek.clients.dblp.DBLPClient.batch_lookup","title":"batch_lookup","text":"<pre><code>batch_lookup(\n    identifiers: List[str], id_type: str\n) -&gt; SearchResult\n</code></pre> <p>Look up multiple papers.</p>"},{"location":"api/database_clients/#paperseek.clients.dblp.DBLPClient.get_supported_fields","title":"get_supported_fields","text":"<pre><code>get_supported_fields() -&gt; List[str]\n</code></pre> <p>Get fields typically provided by DBLP.</p>"},{"location":"api/models/","title":"Data Models","text":"<p>Pydantic models for papers and search results.</p>"},{"location":"api/models/#paper","title":"Paper","text":""},{"location":"api/models/#paperseek.core.models.Paper","title":"Paper","text":"<p>               Bases: <code>BaseModel</code></p> <p>Normalized paper metadata across different databases.</p> <p>This model represents the unified schema for academic papers, with fields mapped from various database-specific formats.</p>"},{"location":"api/models/#paperseek.core.models.Paper-attributes","title":"Attributes","text":""},{"location":"api/models/#paperseek.core.models.Paper.doi","title":"doi  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>doi: Optional[str] = None\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.pmid","title":"pmid  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pmid: Optional[str] = None\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.arxiv_id","title":"arxiv_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arxiv_id: Optional[str] = None\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.title","title":"title  <code>instance-attribute</code>","text":"<pre><code>title: str\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.authors","title":"authors  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>authors: List[Author] = Field(default_factory=list)\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.year","title":"year  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>year: Optional[int] = None\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.venue","title":"venue  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>venue: Optional[str] = None\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.abstract","title":"abstract  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>abstract: Optional[str] = None\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.keywords","title":"keywords  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>keywords: List[str] = Field(default_factory=list)\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.url","title":"url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>url: Optional[str] = None\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.pdf_url","title":"pdf_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pdf_url: Optional[str] = None\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.citation_count","title":"citation_count  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>citation_count: Optional[int] = None\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.source_database","title":"source_database  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source_database: str = Field(\n    description=\"Database this paper was retrieved from\"\n)\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper.retrieved_at","title":"retrieved_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>retrieved_at: datetime = Field(default_factory=now)\n</code></pre>"},{"location":"api/models/#paperseek.core.models.Paper-functions","title":"Functions","text":""},{"location":"api/models/#paperseek.core.models.Paper.get_available_fields","title":"get_available_fields","text":"<pre><code>get_available_fields() -&gt; List[str]\n</code></pre> <p>Get list of fields that have non-None values.</p>"},{"location":"api/models/#author","title":"Author","text":""},{"location":"api/models/#paperseek.core.models.Author","title":"Author","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a paper author.</p>"},{"location":"api/models/#paperseek.core.models.Author-functions","title":"Functions","text":""},{"location":"api/models/#paperseek.core.models.Author.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return the author's name as string representation.</p>"},{"location":"api/models/#searchresult","title":"SearchResult","text":""},{"location":"api/models/#paperseek.core.models.SearchResult","title":"SearchResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for search results with metadata and statistics.</p>"},{"location":"api/models/#paperseek.core.models.SearchResult-attributes","title":"Attributes","text":""},{"location":"api/models/#paperseek.core.models.SearchResult.papers","title":"papers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>papers: List[Paper] = Field(default_factory=list)\n</code></pre>"},{"location":"api/models/#paperseek.core.models.SearchResult.total_results","title":"total_results  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_results: int = 0\n</code></pre>"},{"location":"api/models/#paperseek.core.models.SearchResult.databases_queried","title":"databases_queried  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>databases_queried: List[str] = Field(default_factory=list)\n</code></pre>"},{"location":"api/models/#paperseek.core.models.SearchResult-functions","title":"Functions","text":""},{"location":"api/models/#paperseek.core.models.SearchResult.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the number of papers in the results.</p>"},{"location":"api/models/#paperseek.core.models.SearchResult.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int) -&gt; Paper\n</code></pre><pre><code>__getitem__(index: slice) -&gt; List[Paper]\n</code></pre> <pre><code>__getitem__(index: int | slice) -&gt; Paper | List[Paper]\n</code></pre> <p>Get a paper by index or a slice of papers.</p>"},{"location":"api/models/#paperseek.core.models.SearchResult.add_paper","title":"add_paper","text":"<pre><code>add_paper(paper: Paper) -&gt; None\n</code></pre> <p>Add a paper to the results.</p>"},{"location":"api/models/#paperseek.core.models.SearchResult.extend","title":"extend","text":"<pre><code>extend(papers: List[Paper]) -&gt; None\n</code></pre> <p>Add multiple papers to the results.</p>"},{"location":"api/models/#paperseek.core.models.SearchResult.filter_by_required_fields","title":"filter_by_required_fields","text":"<pre><code>filter_by_required_fields(\n    required_fields: List[str],\n) -&gt; SearchResult\n</code></pre> <p>Filter results to only include papers with all required fields.</p>"},{"location":"api/models/#paperseek.core.models.SearchResult.to_csv","title":"to_csv","text":"<pre><code>to_csv(\n    filename: str,\n    columns: Optional[List[str]] = None,\n    include_field_stats: bool = False,\n) -&gt; None\n</code></pre> <p>Export results to CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to output CSV file</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of columns to include (None = all available)</p> <code>None</code> <code>include_field_stats</code> <code>bool</code> <p>If True, create a separate stats CSV</p> <code>False</code>"},{"location":"api/models/#paperseek.core.models.SearchResult.to_json","title":"to_json","text":"<pre><code>to_json(filename: str, pretty: bool = True) -&gt; None\n</code></pre> <p>Export results to JSON file.</p>"},{"location":"api/models/#paperseek.core.models.SearchResult.to_jsonl","title":"to_jsonl","text":"<pre><code>to_jsonl(filename: str) -&gt; None\n</code></pre> <p>Export results to JSONL file (one JSON object per line).</p>"},{"location":"api/models/#paperseek.core.models.SearchResult.to_bibtex","title":"to_bibtex","text":"<pre><code>to_bibtex(filename: str) -&gt; None\n</code></pre> <p>Export results to BibTeX file.</p>"},{"location":"api/models/#searchfilters","title":"SearchFilters","text":""},{"location":"api/models/#paperseek.core.models.SearchFilters","title":"SearchFilters","text":"<p>               Bases: <code>BaseModel</code></p> <p>Search filter parameters.</p>"},{"location":"api/models/#paperseek.core.models.SearchFilters-functions","title":"Functions","text":""},{"location":"api/models/#paperseek.core.models.SearchFilters.has_identifier_filter","title":"has_identifier_filter","text":"<pre><code>has_identifier_filter() -&gt; bool\n</code></pre> <p>Check if any identifier filter is set.</p>"},{"location":"api/models/#paperseek.core.models.SearchFilters.get_year_range","title":"get_year_range","text":"<pre><code>get_year_range() -&gt; Optional[tuple]\n</code></pre> <p>Get the year range as a tuple.</p>"},{"location":"api/unified_client/","title":"UnifiedSearchClient","text":"<p>The main interface for searching across multiple academic databases.</p>"},{"location":"api/unified_client/#paperseek.UnifiedSearchClient","title":"UnifiedSearchClient","text":"<p>Unified client for searching across multiple academic databases.</p> <p>This client orchestrates searches across CrossRef, OpenAlex, Semantic Scholar, and DOI.org with fallback support and result merging.</p> <p>Examples:</p> <p>Basic usage with default configuration:</p> <pre><code>&gt;&gt;&gt; from paperseek import UnifiedSearchClient, SearchFilters\n&gt;&gt;&gt; client = UnifiedSearchClient()\n&gt;&gt;&gt; filters = SearchFilters(title=\"machine learning\", max_results=10)\n&gt;&gt;&gt; results = client.search(filters)\n&gt;&gt;&gt; print(f\"Found {len(results)} papers\")\n</code></pre> <p>Searching specific databases in parallel:</p> <pre><code>&gt;&gt;&gt; client = UnifiedSearchClient(\n...     databases=[\"arxiv\", \"semantic_scholar\"],\n...     fallback_mode=\"parallel\"\n... )\n&gt;&gt;&gt; filters = SearchFilters(author=\"LeCun\", year=2020)\n&gt;&gt;&gt; results = client.search(filters, databases=[\"arxiv\", \"pubmed\"])\n</code></pre> <p>Using custom configuration:</p> <pre><code>&gt;&gt;&gt; config_dict = {\n...     \"email\": \"researcher@university.edu\",\n...     \"semantic_scholar\": {\"api_key\": \"your-api-key\"}\n... }\n&gt;&gt;&gt; client = UnifiedSearchClient(config_dict=config_dict)\n</code></pre> <p>Sequential search with fallback:</p> <pre><code>&gt;&gt;&gt; client = UnifiedSearchClient(fallback_mode=\"sequential\")\n&gt;&gt;&gt; results = client.search(filters)  # Tries databases in order\n</code></pre> Source code in <code>src/paperseek/core/unified_client.py</code> <pre><code>class UnifiedSearchClient:\n    \"\"\"\n    Unified client for searching across multiple academic databases.\n\n    This client orchestrates searches across CrossRef, OpenAlex,\n    Semantic Scholar, and DOI.org with fallback support and result merging.\n\n    Examples:\n        Basic usage with default configuration:\n\n        &gt;&gt;&gt; from paperseek import UnifiedSearchClient, SearchFilters\n        &gt;&gt;&gt; client = UnifiedSearchClient()\n        &gt;&gt;&gt; filters = SearchFilters(title=\"machine learning\", max_results=10)\n        &gt;&gt;&gt; results = client.search(filters)\n        &gt;&gt;&gt; print(f\"Found {len(results)} papers\")\n\n        Searching specific databases in parallel:\n\n        &gt;&gt;&gt; client = UnifiedSearchClient(\n        ...     databases=[\"arxiv\", \"semantic_scholar\"],\n        ...     fallback_mode=\"parallel\"\n        ... )\n        &gt;&gt;&gt; filters = SearchFilters(author=\"LeCun\", year=2020)\n        &gt;&gt;&gt; results = client.search(filters, databases=[\"arxiv\", \"pubmed\"])\n\n        Using custom configuration:\n\n        &gt;&gt;&gt; config_dict = {\n        ...     \"email\": \"researcher@university.edu\",\n        ...     \"semantic_scholar\": {\"api_key\": \"your-api-key\"}\n        ... }\n        &gt;&gt;&gt; client = UnifiedSearchClient(config_dict=config_dict)\n\n        Sequential search with fallback:\n\n        &gt;&gt;&gt; client = UnifiedSearchClient(fallback_mode=\"sequential\")\n        &gt;&gt;&gt; results = client.search(filters)  # Tries databases in order\n    \"\"\"\n\n    def __init__(\n        self,\n        databases: Optional[List[str]] = None,\n        fallback_mode: str = \"sequential\",\n        config_file: Optional[str] = None,\n        config_dict: Optional[Dict] = None,\n        config: Optional[AcademicSearchConfig] = None,\n    ):\n        \"\"\"\n        Initialize unified search client.\n\n        Args:\n            databases: List of databases to use (default: all enabled)\n            fallback_mode: How to handle multiple databases:\n                - 'sequential': Try databases in order until success\n                - 'parallel': Query all databases in parallel and merge\n                - 'first': Use only the first database\n            config_file: Path to configuration file\n            config_dict: Configuration dictionary\n            config: Pre-configured AcademicSearchConfig object\n        \"\"\"\n        self.logger = get_logger(self.__class__.__name__)\n\n        # Load configuration\n        if config:\n            self.config = config\n        else:\n            self.config = load_config(\n                config_file=config_file, config_dict=config_dict, use_env=True\n            )\n\n        # Set fallback mode\n        self.fallback_mode = fallback_mode or self.config.fallback_mode\n\n        # Initialize database clients\n        self.clients: Dict[str, DatabaseClient] = {}\n        self._init_clients(databases)\n\n        if not self.clients:\n            raise ConfigurationError(\"No database clients are enabled\")\n\n    def _init_clients(self, databases: Optional[List[str]] = None) -&gt; None:\n        \"\"\"Initialize database clients based on configuration.\"\"\"\n        available_clients = {\n            \"crossref\": CrossRefClient,\n            \"openalex\": OpenAlexClient,\n            \"semantic_scholar\": SemanticScholarClient,\n            \"doi\": DOIClient,\n            \"pubmed\": PubMedClient,\n            \"arxiv\": ArXivClient,\n            \"core\": COREClient,\n            \"unpaywall\": UnpaywallClient,\n            \"dblp\": DBLPClient,\n        }\n\n        # Determine which databases to use\n        if databases:\n            db_list = [db.lower() for db in databases]\n        else:\n            # Use all enabled databases\n            db_list = [\n                name\n                for name in available_clients.keys()\n                if self.config.get_database_config(name).enabled\n            ]\n\n        # Initialize clients\n        for db_name in db_list:\n            if db_name not in available_clients:\n                self.logger.warning(f\"Unknown database: {db_name}\")\n                continue\n\n            db_config = self.config.get_database_config(db_name)\n            if not db_config.enabled:\n                self.logger.info(f\"Database {db_name} is disabled in config\")\n                continue\n\n            try:\n                client_class = available_clients[db_name]\n                self.clients[db_name] = client_class(\n                    config=db_config, email=self.config.email, user_agent=self.config.user_agent\n                )\n                self.logger.info(f\"Initialized {db_name} client\")\n            except Exception as e:\n                self.logger.error(f\"Failed to initialize {db_name} client: {e}\")\n\n    def search(\n        self,\n        venue: Optional[str] = None,\n        year: Optional[int] = None,\n        year_range: Optional[tuple] = None,\n        title: Optional[str] = None,\n        author: Optional[str] = None,\n        doi: Optional[str] = None,\n        keywords: Optional[List[str]] = None,\n        required_fields: Optional[List[str]] = None,\n        max_results: int = 100,\n        **kwargs,\n    ) -&gt; SearchResult:\n        \"\"\"\n        Search across configured databases.\n\n        Args:\n            venue: Conference or journal name\n            year: Publication year\n            year_range: Tuple of (start_year, end_year)\n            title: Paper title\n            author: Author name\n            doi: Digital Object Identifier\n            keywords: List of keywords\n            required_fields: Fields that must be present in results\n            max_results: Maximum number of results\n            **kwargs: Additional search parameters\n\n        Returns:\n            SearchResult object with combined results\n\n        Examples:\n            Search by title:\n\n            &gt;&gt;&gt; client = UnifiedSearchClient()\n            &gt;&gt;&gt; results = client.search(title=\"attention is all you need\")\n            &gt;&gt;&gt; print(f\"Found {len(results)} papers\")\n\n            Search by author and year:\n\n            &gt;&gt;&gt; results = client.search(author=\"Hinton\", year=2020)\n            &gt;&gt;&gt; for paper in results.papers:\n            ...     print(f\"{paper.title} ({paper.year})\")\n\n            Search with multiple criteria:\n\n            &gt;&gt;&gt; results = client.search(\n            ...     venue=\"NeurIPS\",\n            ...     year_range=(2018, 2022),\n            ...     keywords=[\"deep learning\", \"transformers\"],\n            ...     max_results=50\n            ... )\n\n            Search by DOI:\n\n            &gt;&gt;&gt; results = client.search(doi=\"10.1038/nature14539\")\n            &gt;&gt;&gt; if results.papers:\n            ...     paper = results.papers[0]\n            ...     print(f\"Title: {paper.title}\")\n        \"\"\"\n        # Build search filters\n        filters = SearchFilters(\n            venue=venue,\n            year=year,\n            title=title,\n            author=author,\n            doi=doi,\n            keywords=keywords,\n            required_fields=required_fields,\n            max_results=max_results,\n        )\n\n        # Handle year range\n        if year_range:\n            filters.year_start = year_range[0]\n            filters.year_end = year_range[1]\n\n        # Additional kwargs\n        for key, value in kwargs.items():\n            if hasattr(filters, key):\n                setattr(filters, key, value)\n\n        return self.search_with_filters(filters)\n\n    def search_with_filters(self, filters: SearchFilters) -&gt; SearchResult:\n        \"\"\"\n        Search with a SearchFilters object.\n\n        Args:\n            filters: SearchFilters object\n\n        Returns:\n            SearchResult object\n        \"\"\"\n        self.logger.info(\n            f\"Searching with mode '{self.fallback_mode}' \" f\"across {len(self.clients)} databases\"\n        )\n\n        if self.fallback_mode == \"parallel\":\n            return self._search_parallel(filters)\n        elif self.fallback_mode == \"sequential\":\n            return self._search_sequential(filters)\n        elif self.fallback_mode == \"first\":\n            return self._search_first(filters)\n        else:\n            raise ConfigurationError(f\"Invalid fallback_mode: {self.fallback_mode}\")\n\n    def _search_parallel(self, filters: SearchFilters) -&gt; SearchResult:\n        \"\"\"Search all databases in parallel and merge results.\"\"\"\n        results = SearchResult(query_info={\"filters\": filters.model_dump()}, databases_queried=[])\n\n        # Execute searches in parallel\n        with ThreadPoolExecutor(max_workers=len(self.clients)) as executor:\n            future_to_db = {\n                executor.submit(client.search, filters): db_name\n                for db_name, client in self.clients.items()\n            }\n\n            for future in as_completed(future_to_db):\n                db_name = future_to_db[future]\n                try:\n                    db_result = future.result()\n                    results.databases_queried.append(db_name)\n                    results.extend(db_result.papers)\n                    self.logger.info(f\"Got {len(db_result.papers)} results from {db_name}\")\n                except Exception as e:\n                    self.logger.error(f\"Search failed for {db_name}: {e}\")\n                    if self.config.fail_fast:\n                        raise SearchError(f\"Search failed for {db_name}: {e}\")\n\n        # Deduplicate by DOI\n        results = self._deduplicate_results(results)\n\n        # Filter by required fields if specified\n        if filters.required_fields:\n            results = results.filter_by_required_fields(filters.required_fields)\n\n        return results\n\n    def _search_sequential(self, filters: SearchFilters) -&gt; SearchResult:\n        \"\"\"Search databases sequentially with fallback.\"\"\"\n        results = SearchResult(query_info={\"filters\": filters.model_dump()}, databases_queried=[])\n\n        for db_name, client in self.clients.items():\n            try:\n                self.logger.info(f\"Searching {db_name}...\")\n                db_result = client.search(filters)\n                results.databases_queried.append(db_name)\n                results.extend(db_result.papers)\n\n                self.logger.info(f\"Got {len(db_result.papers)} results from {db_name}\")\n\n                # If we got results and not in fallback mode, we're done\n                if db_result.papers and not self.config.fail_fast:\n                    break\n\n            except Exception as e:\n                self.logger.error(f\"Search failed for {db_name}: {e}\")\n                if self.config.fail_fast:\n                    raise SearchError(f\"Search failed for {db_name}: {e}\")\n                # Continue to next database\n\n        # Deduplicate\n        results = self._deduplicate_results(results)\n\n        # Filter by required fields\n        if filters.required_fields:\n            results = results.filter_by_required_fields(filters.required_fields)\n\n        return results\n\n    def _search_first(self, filters: SearchFilters) -&gt; SearchResult:\n        \"\"\"Search only the first database.\"\"\"\n        if not self.clients:\n            raise SearchError(\"No databases configured\")\n\n        db_name = list(self.clients.keys())[0]\n        client = self.clients[db_name]\n\n        try:\n            self.logger.info(f\"Searching {db_name}...\")\n            results = client.search(filters)\n            results.databases_queried = [db_name]\n\n            # Filter by required fields\n            if filters.required_fields:\n                results = results.filter_by_required_fields(filters.required_fields)\n\n            return results\n        except Exception as e:\n            raise SearchError(f\"Search failed for {db_name}: {e}\")\n\n    def _deduplicate_results(self, results: SearchResult) -&gt; SearchResult:\n        \"\"\"\n        Deduplicate papers by DOI and other identifiers.\n\n        Args:\n            results: SearchResult with potentially duplicate papers\n\n        Returns:\n            Deduplicated SearchResult\n        \"\"\"\n        seen_ids = set()\n        unique_papers = []\n\n        for paper in results.papers:\n            # Create identifier tuple\n            paper_id = (\n                paper.doi or \"\",\n                paper.pmid or \"\",\n                paper.arxiv_id or \"\",\n                paper.title.lower() if paper.title else \"\",\n            )\n\n            # Check if we've seen this paper\n            if paper_id not in seen_ids and any(paper_id):\n                seen_ids.add(paper_id)\n                unique_papers.append(paper)\n\n        results.papers = unique_papers\n        results.total_results = len(unique_papers)\n\n        self.logger.info(\n            f\"Deduplicated {len(results.papers)} papers \" f\"from {len(seen_ids)} unique identifiers\"\n        )\n\n        return results\n\n    def get_by_doi(self, doi: str, databases: Optional[List[str]] = None) -&gt; Optional[Paper]:\n        \"\"\"\n        Get a paper by DOI from specified databases.\n\n        Args:\n            doi: Digital Object Identifier\n            databases: List of databases to try (default: all)\n\n        Returns:\n            Paper object or None\n        \"\"\"\n        db_list = databases or list(self.clients.keys())\n\n        for db_name in db_list:\n            if db_name not in self.clients:\n                continue\n\n            try:\n                paper = self.clients[db_name].get_by_doi(doi)\n                if paper:\n                    self.logger.info(f\"Found paper with DOI {doi} in {db_name}\")\n                    return paper\n            except Exception as e:\n                self.logger.warning(f\"Failed to get DOI {doi} from {db_name}: {e}\")\n\n        return None\n\n    def batch_lookup(\n        self, identifiers: List[str], id_type: str = \"doi\", databases: Optional[List[str]] = None\n    ) -&gt; SearchResult:\n        \"\"\"\n        Look up multiple papers by identifier.\n\n        Args:\n            identifiers: List of identifiers\n            id_type: Type of identifier (doi, pmid, arxiv, etc.)\n            databases: List of databases to use (default: all)\n\n        Returns:\n            SearchResult with found papers\n        \"\"\"\n        db_list = databases or list(self.clients.keys())\n\n        results = SearchResult(\n            query_info={\"identifiers\": identifiers, \"id_type\": id_type}, databases_queried=[]\n        )\n\n        for db_name in db_list:\n            if db_name not in self.clients:\n                continue\n\n            try:\n                db_result = self.clients[db_name].batch_lookup(identifiers, id_type)\n                results.databases_queried.append(db_name)\n                results.extend(db_result.papers)\n                self.logger.info(f\"Got {len(db_result.papers)} results from {db_name}\")\n            except Exception as e:\n                self.logger.error(f\"Batch lookup failed for {db_name}: {e}\")\n\n        # Deduplicate\n        results = self._deduplicate_results(results)\n\n        return results\n\n    def get_client(self, database: str) -&gt; Optional[DatabaseClient]:\n        \"\"\"\n        Get a specific database client.\n\n        Args:\n            database: Database name\n\n        Returns:\n            DatabaseClient instance or None\n        \"\"\"\n        return self.clients.get(database.lower())\n\n    def close(self) -&gt; None:\n        \"\"\"Close all database client sessions.\"\"\"\n        for client in self.clients.values():\n            try:\n                client.close()\n            except Exception as e:\n                self.logger.error(f\"Error closing client: {e}\")\n\n    def __enter__(self) -&gt; \"UnifiedSearchClient\":\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        \"\"\"Context manager exit.\"\"\"\n        self.close()\n</code></pre>"},{"location":"api/unified_client/#paperseek.UnifiedSearchClient-functions","title":"Functions","text":""},{"location":"api/unified_client/#paperseek.UnifiedSearchClient.__init__","title":"__init__","text":"<pre><code>__init__(\n    databases: Optional[List[str]] = None,\n    fallback_mode: str = \"sequential\",\n    config_file: Optional[str] = None,\n    config_dict: Optional[Dict] = None,\n    config: Optional[AcademicSearchConfig] = None,\n)\n</code></pre> <p>Initialize unified search client.</p> <p>Parameters:</p> Name Type Description Default <code>databases</code> <code>Optional[List[str]]</code> <p>List of databases to use (default: all enabled)</p> <code>None</code> <code>fallback_mode</code> <code>str</code> <p>How to handle multiple databases: - 'sequential': Try databases in order until success - 'parallel': Query all databases in parallel and merge - 'first': Use only the first database</p> <code>'sequential'</code> <code>config_file</code> <code>Optional[str]</code> <p>Path to configuration file</p> <code>None</code> <code>config_dict</code> <code>Optional[Dict]</code> <p>Configuration dictionary</p> <code>None</code> <code>config</code> <code>Optional[AcademicSearchConfig]</code> <p>Pre-configured AcademicSearchConfig object</p> <code>None</code> Source code in <code>src/paperseek/core/unified_client.py</code> <pre><code>def __init__(\n    self,\n    databases: Optional[List[str]] = None,\n    fallback_mode: str = \"sequential\",\n    config_file: Optional[str] = None,\n    config_dict: Optional[Dict] = None,\n    config: Optional[AcademicSearchConfig] = None,\n):\n    \"\"\"\n    Initialize unified search client.\n\n    Args:\n        databases: List of databases to use (default: all enabled)\n        fallback_mode: How to handle multiple databases:\n            - 'sequential': Try databases in order until success\n            - 'parallel': Query all databases in parallel and merge\n            - 'first': Use only the first database\n        config_file: Path to configuration file\n        config_dict: Configuration dictionary\n        config: Pre-configured AcademicSearchConfig object\n    \"\"\"\n    self.logger = get_logger(self.__class__.__name__)\n\n    # Load configuration\n    if config:\n        self.config = config\n    else:\n        self.config = load_config(\n            config_file=config_file, config_dict=config_dict, use_env=True\n        )\n\n    # Set fallback mode\n    self.fallback_mode = fallback_mode or self.config.fallback_mode\n\n    # Initialize database clients\n    self.clients: Dict[str, DatabaseClient] = {}\n    self._init_clients(databases)\n\n    if not self.clients:\n        raise ConfigurationError(\"No database clients are enabled\")\n</code></pre>"},{"location":"api/unified_client/#paperseek.UnifiedSearchClient.search","title":"search","text":"<pre><code>search(\n    venue: Optional[str] = None,\n    year: Optional[int] = None,\n    year_range: Optional[tuple] = None,\n    title: Optional[str] = None,\n    author: Optional[str] = None,\n    doi: Optional[str] = None,\n    keywords: Optional[List[str]] = None,\n    required_fields: Optional[List[str]] = None,\n    max_results: int = 100,\n    **kwargs\n) -&gt; SearchResult\n</code></pre> <p>Search across configured databases.</p> <p>Parameters:</p> Name Type Description Default <code>venue</code> <code>Optional[str]</code> <p>Conference or journal name</p> <code>None</code> <code>year</code> <code>Optional[int]</code> <p>Publication year</p> <code>None</code> <code>year_range</code> <code>Optional[tuple]</code> <p>Tuple of (start_year, end_year)</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>Paper title</p> <code>None</code> <code>author</code> <code>Optional[str]</code> <p>Author name</p> <code>None</code> <code>doi</code> <code>Optional[str]</code> <p>Digital Object Identifier</p> <code>None</code> <code>keywords</code> <code>Optional[List[str]]</code> <p>List of keywords</p> <code>None</code> <code>required_fields</code> <code>Optional[List[str]]</code> <p>Fields that must be present in results</p> <code>None</code> <code>max_results</code> <code>int</code> <p>Maximum number of results</p> <code>100</code> <code>**kwargs</code> <p>Additional search parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object with combined results</p> <p>Examples:</p> <p>Search by title:</p> <pre><code>&gt;&gt;&gt; client = UnifiedSearchClient()\n&gt;&gt;&gt; results = client.search(title=\"attention is all you need\")\n&gt;&gt;&gt; print(f\"Found {len(results)} papers\")\n</code></pre> <p>Search by author and year:</p> <pre><code>&gt;&gt;&gt; results = client.search(author=\"Hinton\", year=2020)\n&gt;&gt;&gt; for paper in results.papers:\n...     print(f\"{paper.title} ({paper.year})\")\n</code></pre> <p>Search with multiple criteria:</p> <pre><code>&gt;&gt;&gt; results = client.search(\n...     venue=\"NeurIPS\",\n...     year_range=(2018, 2022),\n...     keywords=[\"deep learning\", \"transformers\"],\n...     max_results=50\n... )\n</code></pre> <p>Search by DOI:</p> <pre><code>&gt;&gt;&gt; results = client.search(doi=\"10.1038/nature14539\")\n&gt;&gt;&gt; if results.papers:\n...     paper = results.papers[0]\n...     print(f\"Title: {paper.title}\")\n</code></pre> Source code in <code>src/paperseek/core/unified_client.py</code> <pre><code>def search(\n    self,\n    venue: Optional[str] = None,\n    year: Optional[int] = None,\n    year_range: Optional[tuple] = None,\n    title: Optional[str] = None,\n    author: Optional[str] = None,\n    doi: Optional[str] = None,\n    keywords: Optional[List[str]] = None,\n    required_fields: Optional[List[str]] = None,\n    max_results: int = 100,\n    **kwargs,\n) -&gt; SearchResult:\n    \"\"\"\n    Search across configured databases.\n\n    Args:\n        venue: Conference or journal name\n        year: Publication year\n        year_range: Tuple of (start_year, end_year)\n        title: Paper title\n        author: Author name\n        doi: Digital Object Identifier\n        keywords: List of keywords\n        required_fields: Fields that must be present in results\n        max_results: Maximum number of results\n        **kwargs: Additional search parameters\n\n    Returns:\n        SearchResult object with combined results\n\n    Examples:\n        Search by title:\n\n        &gt;&gt;&gt; client = UnifiedSearchClient()\n        &gt;&gt;&gt; results = client.search(title=\"attention is all you need\")\n        &gt;&gt;&gt; print(f\"Found {len(results)} papers\")\n\n        Search by author and year:\n\n        &gt;&gt;&gt; results = client.search(author=\"Hinton\", year=2020)\n        &gt;&gt;&gt; for paper in results.papers:\n        ...     print(f\"{paper.title} ({paper.year})\")\n\n        Search with multiple criteria:\n\n        &gt;&gt;&gt; results = client.search(\n        ...     venue=\"NeurIPS\",\n        ...     year_range=(2018, 2022),\n        ...     keywords=[\"deep learning\", \"transformers\"],\n        ...     max_results=50\n        ... )\n\n        Search by DOI:\n\n        &gt;&gt;&gt; results = client.search(doi=\"10.1038/nature14539\")\n        &gt;&gt;&gt; if results.papers:\n        ...     paper = results.papers[0]\n        ...     print(f\"Title: {paper.title}\")\n    \"\"\"\n    # Build search filters\n    filters = SearchFilters(\n        venue=venue,\n        year=year,\n        title=title,\n        author=author,\n        doi=doi,\n        keywords=keywords,\n        required_fields=required_fields,\n        max_results=max_results,\n    )\n\n    # Handle year range\n    if year_range:\n        filters.year_start = year_range[0]\n        filters.year_end = year_range[1]\n\n    # Additional kwargs\n    for key, value in kwargs.items():\n        if hasattr(filters, key):\n            setattr(filters, key, value)\n\n    return self.search_with_filters(filters)\n</code></pre>"},{"location":"api/unified_client/#paperseek.UnifiedSearchClient.search_with_filters","title":"search_with_filters","text":"<pre><code>search_with_filters(filters: SearchFilters) -&gt; SearchResult\n</code></pre> <p>Search with a SearchFilters object.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>SearchFilters</code> <p>SearchFilters object</p> required <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult object</p> Source code in <code>src/paperseek/core/unified_client.py</code> <pre><code>def search_with_filters(self, filters: SearchFilters) -&gt; SearchResult:\n    \"\"\"\n    Search with a SearchFilters object.\n\n    Args:\n        filters: SearchFilters object\n\n    Returns:\n        SearchResult object\n    \"\"\"\n    self.logger.info(\n        f\"Searching with mode '{self.fallback_mode}' \" f\"across {len(self.clients)} databases\"\n    )\n\n    if self.fallback_mode == \"parallel\":\n        return self._search_parallel(filters)\n    elif self.fallback_mode == \"sequential\":\n        return self._search_sequential(filters)\n    elif self.fallback_mode == \"first\":\n        return self._search_first(filters)\n    else:\n        raise ConfigurationError(f\"Invalid fallback_mode: {self.fallback_mode}\")\n</code></pre>"},{"location":"api/utilities/","title":"Utilities","text":"<p>Utility classes for PDF downloading and rate limiting.</p>"},{"location":"api/utilities/#pdfdownloader","title":"PDFDownloader","text":""},{"location":"api/utilities/#paperseek.utils.pdf_downloader.PDFDownloader","title":"PDFDownloader","text":"<p>Polite PDF downloader for academic papers.</p> <p>Features: - Conservative rate limiting (1 download per 3 seconds by default) - Respects HTTP headers and content types - Proper User-Agent headers - Timeout handling - File size limits - Resume capability - Progress tracking - Duplicate detection</p>"},{"location":"api/utilities/#paperseek.utils.pdf_downloader.PDFDownloader-functions","title":"Functions","text":""},{"location":"api/utilities/#paperseek.utils.pdf_downloader.PDFDownloader.__init__","title":"__init__","text":"<pre><code>__init__(\n    download_dir: str = \"./downloads\",\n    rate_limit_seconds: float = 3.0,\n    timeout: int = 60,\n    max_file_size_mb: int = 50,\n    user_agent: str = \"AcademicSearchUnified-PDFDownloader/0.1.0\",\n    email: Optional[str] = None,\n    overwrite: bool = False,\n    verify_ssl: bool = True,\n)\n</code></pre> <p>Initialize PDF downloader.</p> <p>Parameters:</p> Name Type Description Default <code>download_dir</code> <code>str</code> <p>Directory to save PDFs</p> <code>'./downloads'</code> <code>rate_limit_seconds</code> <code>float</code> <p>Minimum seconds between downloads (default: 3.0)</p> <code>3.0</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>60</code> <code>max_file_size_mb</code> <code>int</code> <p>Maximum file size to download in MB</p> <code>50</code> <code>user_agent</code> <code>str</code> <p>User-Agent string</p> <code>'AcademicSearchUnified-PDFDownloader/0.1.0'</code> <code>email</code> <code>Optional[str]</code> <p>Email for polite requests</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing files</p> <code>False</code> <code>verify_ssl</code> <code>bool</code> <p>Whether to verify SSL certificates</p> <code>True</code>"},{"location":"api/utilities/#paperseek.utils.pdf_downloader.PDFDownloader.download_paper","title":"download_paper","text":"<pre><code>download_paper(\n    paper: Paper,\n    filename: Optional[str] = None,\n    subdirectory: Optional[str] = None,\n) -&gt; Optional[Path]\n</code></pre> <p>Download PDF for a single paper.</p> <p>Parameters:</p> Name Type Description Default <code>paper</code> <code>Paper</code> <p>Paper object with pdf_url</p> required <code>filename</code> <code>Optional[str]</code> <p>Optional custom filename</p> <code>None</code> <code>subdirectory</code> <code>Optional[str]</code> <p>Optional subdirectory within download_dir</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Path]</code> <p>Path to downloaded file, or None if download failed</p>"},{"location":"api/utilities/#paperseek.utils.pdf_downloader.PDFDownloader.download_papers","title":"download_papers","text":"<pre><code>download_papers(\n    papers: List[Paper],\n    subdirectory: Optional[str] = None,\n    max_downloads: Optional[int] = None,\n) -&gt; Dict[str, Path]\n</code></pre> <p>Download PDFs for multiple papers.</p> <p>Parameters:</p> Name Type Description Default <code>papers</code> <code>List[Paper]</code> <p>List of Paper objects</p> required <code>subdirectory</code> <code>Optional[str]</code> <p>Optional subdirectory within download_dir</p> <code>None</code> <code>max_downloads</code> <code>Optional[int]</code> <p>Maximum number of PDFs to download</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Path]</code> <p>Dictionary mapping paper titles to downloaded file paths</p>"},{"location":"api/utilities/#paperseek.utils.pdf_downloader.PDFDownloader.get_statistics","title":"get_statistics","text":"<pre><code>get_statistics() -&gt; Dict[str, float]\n</code></pre> <p>Get download statistics.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with download statistics</p>"},{"location":"api/utilities/#ratelimiter","title":"RateLimiter","text":""},{"location":"api/utilities/#paperseek.utils.rate_limiter.RateLimiter","title":"RateLimiter","text":"<p>Thread-safe rate limiter for API requests.</p> <p>Supports both per-second and per-minute rate limits.</p>"},{"location":"api/utilities/#paperseek.utils.rate_limiter.RateLimiter-functions","title":"Functions","text":""},{"location":"api/utilities/#paperseek.utils.rate_limiter.RateLimiter.__init__","title":"__init__","text":"<pre><code>__init__(\n    requests_per_second: Optional[float] = None,\n    requests_per_minute: Optional[float] = None,\n)\n</code></pre> <p>Initialize rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>requests_per_second</code> <code>Optional[float]</code> <p>Maximum requests per second</p> <code>None</code> <code>requests_per_minute</code> <code>Optional[float]</code> <p>Maximum requests per minute</p> <code>None</code>"},{"location":"api/utilities/#paperseek.utils.rate_limiter.RateLimiter.wait_if_needed","title":"wait_if_needed","text":"<pre><code>wait_if_needed() -&gt; None\n</code></pre> <p>Wait if rate limit would be exceeded.</p>"},{"location":"api/utilities/#databaseratelimiter","title":"DatabaseRateLimiter","text":""},{"location":"api/utilities/#paperseek.utils.rate_limiter.DatabaseRateLimiter","title":"DatabaseRateLimiter","text":"<p>Manages rate limiters for multiple databases.</p>"},{"location":"api/utilities/#paperseek.utils.rate_limiter.DatabaseRateLimiter-functions","title":"Functions","text":""},{"location":"api/utilities/#paperseek.utils.rate_limiter.DatabaseRateLimiter.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize database rate limiter manager.</p>"},{"location":"api/utilities/#paperseek.utils.rate_limiter.DatabaseRateLimiter.add_database","title":"add_database","text":"<pre><code>add_database(\n    database: str,\n    requests_per_second: Optional[float] = None,\n    requests_per_minute: Optional[float] = None,\n) -&gt; None\n</code></pre> <p>Add or update rate limiter for a database.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>Database name</p> required <code>requests_per_second</code> <code>Optional[float]</code> <p>Maximum requests per second</p> <code>None</code> <code>requests_per_minute</code> <code>Optional[float]</code> <p>Maximum requests per minute</p> <code>None</code>"},{"location":"api/utilities/#paperseek.utils.rate_limiter.DatabaseRateLimiter.wait_if_needed","title":"wait_if_needed","text":"<pre><code>wait_if_needed(database: str) -&gt; None\n</code></pre> <p>Wait if rate limit would be exceeded for the database.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>Database name</p> required"},{"location":"api/utilities/#paperseek.utils.rate_limiter.DatabaseRateLimiter.get_limiter","title":"get_limiter","text":"<pre><code>get_limiter(database: str) -&gt; Optional[RateLimiter]\n</code></pre> <p>Get rate limiter for a database.</p>"}]}